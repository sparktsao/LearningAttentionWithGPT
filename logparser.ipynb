{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "position encoder pe shape torch.Size([MAX_LEN, EMBEDDING_DIM])\n",
      "position encoder pe shape torch.Size([MAX_LEN, EMBEDDING_DIM])\n",
      "[Transformer] src.shape: torch.Size([BATCH_SIZE, SEQ_LEN]), trg.shape: torch.Size([BATCH_SIZE, SEQ_LEN])\n",
      "[make_src_mask] mask.shape: torch.Size([BATCH_SIZE, 1, 1, SEQ_LEN])\n",
      "[make_trg_mask] mask.shape: torch.Size([BATCH_SIZE, 1, SEQ_LEN, SEQ_LEN])\n",
      "[TransformerEncoder] Input x.shape: torch.Size([BATCH_SIZE, SEQ_LEN])\n",
      "[TransformerEncoder] After word embedding x.shape: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])\n",
      "[PositionalEncoding] Input x.shape: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM]), Adding positional encodings for sequence length and embedding size. peshape torch.Size([1, MAX_LEN, EMBEDDING_DIM])\n",
      "while adding, pe[:, :x.size(1)]: torch.Size([1, SEQ_LEN, EMBEDDING_DIM])\n",
      "[PositionalEncoding] Output shape: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])\n",
      "\n",
      "[TransformerEncoder] Processing layer 1\n",
      "[TransformerBlock] Starting block processing. Input dimensions - value: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM]), key: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM]), query: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])\n",
      "[MultiHeadAttention:1] Input shapes (V, K, Q) - values: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM]), keys: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM]), queries: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])\n",
      "[MultiHeadAttention:2] After linear projections V/K/Q multihead- values: torch.Size([BATCH_SIZE, SEQ_LEN, HEAD_COUNT, EMBEDDING_PER_HEAD]), keys: torch.Size([BATCH_SIZE, SEQ_LEN, HEAD_COUNT, EMBEDDING_PER_HEAD]), queries: torch.Size([BATCH_SIZE, SEQ_LEN, HEAD_COUNT, EMBEDDING_PER_HEAD])\n",
      "[MultiHeadAttention:3] Energy = Q * K (raw attention scores) shape: torch.Size([BATCH_SIZE, HEAD_COUNT, SEQ_LEN, SEQ_LEN])\n",
      "[MultiHeadAttention:4] Applying mask with shape: torch.Size([BATCH_SIZE, 1, 1, SEQ_LEN])\n",
      "[MultiHeadAttention:5] Attention SoftMax weights shape: torch.Size([BATCH_SIZE, HEAD_COUNT, SEQ_LEN, SEQ_LEN])\n",
      "[MultiHeadAttention:6] Attention * V = Output shape after weighted sum: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])\n",
      "[TransformerBlock] After attention and residual connection: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])\n",
      "[FeedForward] Input shape: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM]), Performing feedforward transformations.\n",
      "[FeedForward] Output shape: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])\n",
      "[TransformerBlock] After feedforward and residual connection: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])\n",
      "\n",
      "[TransformerEncoder] Processing layer 2\n",
      "[TransformerBlock] Starting block processing. Input dimensions - value: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM]), key: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM]), query: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])\n",
      "[MultiHeadAttention:1] Input shapes (V, K, Q) - values: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM]), keys: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM]), queries: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])\n",
      "[MultiHeadAttention:2] After linear projections V/K/Q multihead- values: torch.Size([BATCH_SIZE, SEQ_LEN, HEAD_COUNT, EMBEDDING_PER_HEAD]), keys: torch.Size([BATCH_SIZE, SEQ_LEN, HEAD_COUNT, EMBEDDING_PER_HEAD]), queries: torch.Size([BATCH_SIZE, SEQ_LEN, HEAD_COUNT, EMBEDDING_PER_HEAD])\n",
      "[MultiHeadAttention:3] Energy = Q * K (raw attention scores) shape: torch.Size([BATCH_SIZE, HEAD_COUNT, SEQ_LEN, SEQ_LEN])\n",
      "[MultiHeadAttention:4] Applying mask with shape: torch.Size([BATCH_SIZE, 1, 1, SEQ_LEN])\n",
      "[MultiHeadAttention:5] Attention SoftMax weights shape: torch.Size([BATCH_SIZE, HEAD_COUNT, SEQ_LEN, SEQ_LEN])\n",
      "[MultiHeadAttention:6] Attention * V = Output shape after weighted sum: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])\n",
      "[TransformerBlock] After attention and residual connection: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])\n",
      "[FeedForward] Input shape: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM]), Performing feedforward transformations.\n",
      "[FeedForward] Output shape: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])\n",
      "[TransformerBlock] After feedforward and residual connection: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])\n",
      "\n",
      "[TransformerEncoder] Processing layer 3\n",
      "[TransformerBlock] Starting block processing. Input dimensions - value: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM]), key: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM]), query: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])\n",
      "[MultiHeadAttention:1] Input shapes (V, K, Q) - values: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM]), keys: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM]), queries: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])\n",
      "[MultiHeadAttention:2] After linear projections V/K/Q multihead- values: torch.Size([BATCH_SIZE, SEQ_LEN, HEAD_COUNT, EMBEDDING_PER_HEAD]), keys: torch.Size([BATCH_SIZE, SEQ_LEN, HEAD_COUNT, EMBEDDING_PER_HEAD]), queries: torch.Size([BATCH_SIZE, SEQ_LEN, HEAD_COUNT, EMBEDDING_PER_HEAD])\n",
      "[MultiHeadAttention:3] Energy = Q * K (raw attention scores) shape: torch.Size([BATCH_SIZE, HEAD_COUNT, SEQ_LEN, SEQ_LEN])\n",
      "[MultiHeadAttention:4] Applying mask with shape: torch.Size([BATCH_SIZE, 1, 1, SEQ_LEN])\n",
      "[MultiHeadAttention:5] Attention SoftMax weights shape: torch.Size([BATCH_SIZE, HEAD_COUNT, SEQ_LEN, SEQ_LEN])\n",
      "[MultiHeadAttention:6] Attention * V = Output shape after weighted sum: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])\n",
      "[TransformerBlock] After attention and residual connection: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])\n",
      "[FeedForward] Input shape: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM]), Performing feedforward transformations.\n",
      "[FeedForward] Output shape: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])\n",
      "[TransformerBlock] After feedforward and residual connection: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])\n",
      "\n",
      "[TransformerEncoder] Processing layer 4\n",
      "[TransformerBlock] Starting block processing. Input dimensions - value: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM]), key: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM]), query: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])\n",
      "[MultiHeadAttention:1] Input shapes (V, K, Q) - values: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM]), keys: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM]), queries: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])\n",
      "[MultiHeadAttention:2] After linear projections V/K/Q multihead- values: torch.Size([BATCH_SIZE, SEQ_LEN, HEAD_COUNT, EMBEDDING_PER_HEAD]), keys: torch.Size([BATCH_SIZE, SEQ_LEN, HEAD_COUNT, EMBEDDING_PER_HEAD]), queries: torch.Size([BATCH_SIZE, SEQ_LEN, HEAD_COUNT, EMBEDDING_PER_HEAD])\n",
      "[MultiHeadAttention:3] Energy = Q * K (raw attention scores) shape: torch.Size([BATCH_SIZE, HEAD_COUNT, SEQ_LEN, SEQ_LEN])\n",
      "[MultiHeadAttention:4] Applying mask with shape: torch.Size([BATCH_SIZE, 1, 1, SEQ_LEN])\n",
      "[MultiHeadAttention:5] Attention SoftMax weights shape: torch.Size([BATCH_SIZE, HEAD_COUNT, SEQ_LEN, SEQ_LEN])\n",
      "[MultiHeadAttention:6] Attention * V = Output shape after weighted sum: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])\n",
      "[TransformerBlock] After attention and residual connection: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])\n",
      "[FeedForward] Input shape: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM]), Performing feedforward transformations.\n",
      "[FeedForward] Output shape: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])\n",
      "[TransformerBlock] After feedforward and residual connection: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])\n",
      "\n",
      "[TransformerEncoder] Processing layer 5\n",
      "[TransformerBlock] Starting block processing. Input dimensions - value: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM]), key: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM]), query: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])\n",
      "[MultiHeadAttention:1] Input shapes (V, K, Q) - values: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM]), keys: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM]), queries: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])\n",
      "[MultiHeadAttention:2] After linear projections V/K/Q multihead- values: torch.Size([BATCH_SIZE, SEQ_LEN, HEAD_COUNT, EMBEDDING_PER_HEAD]), keys: torch.Size([BATCH_SIZE, SEQ_LEN, HEAD_COUNT, EMBEDDING_PER_HEAD]), queries: torch.Size([BATCH_SIZE, SEQ_LEN, HEAD_COUNT, EMBEDDING_PER_HEAD])\n",
      "[MultiHeadAttention:3] Energy = Q * K (raw attention scores) shape: torch.Size([BATCH_SIZE, HEAD_COUNT, SEQ_LEN, SEQ_LEN])\n",
      "[MultiHeadAttention:4] Applying mask with shape: torch.Size([BATCH_SIZE, 1, 1, SEQ_LEN])\n",
      "[MultiHeadAttention:5] Attention SoftMax weights shape: torch.Size([BATCH_SIZE, HEAD_COUNT, SEQ_LEN, SEQ_LEN])\n",
      "[MultiHeadAttention:6] Attention * V = Output shape after weighted sum: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])\n",
      "[TransformerBlock] After attention and residual connection: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])\n",
      "[FeedForward] Input shape: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM]), Performing feedforward transformations.\n",
      "[FeedForward] Output shape: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])\n",
      "[TransformerBlock] After feedforward and residual connection: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])\n",
      "\n",
      "[TransformerEncoder] Processing layer 6\n",
      "[TransformerBlock] Starting block processing. Input dimensions - value: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM]), key: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM]), query: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])\n",
      "[MultiHeadAttention:1] Input shapes (V, K, Q) - values: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM]), keys: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM]), queries: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])\n",
      "[MultiHeadAttention:2] After linear projections V/K/Q multihead- values: torch.Size([BATCH_SIZE, SEQ_LEN, HEAD_COUNT, EMBEDDING_PER_HEAD]), keys: torch.Size([BATCH_SIZE, SEQ_LEN, HEAD_COUNT, EMBEDDING_PER_HEAD]), queries: torch.Size([BATCH_SIZE, SEQ_LEN, HEAD_COUNT, EMBEDDING_PER_HEAD])\n",
      "[MultiHeadAttention:3] Energy = Q * K (raw attention scores) shape: torch.Size([BATCH_SIZE, HEAD_COUNT, SEQ_LEN, SEQ_LEN])\n",
      "[MultiHeadAttention:4] Applying mask with shape: torch.Size([BATCH_SIZE, 1, 1, SEQ_LEN])\n",
      "[MultiHeadAttention:5] Attention SoftMax weights shape: torch.Size([BATCH_SIZE, HEAD_COUNT, SEQ_LEN, SEQ_LEN])\n",
      "[MultiHeadAttention:6] Attention * V = Output shape after weighted sum: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])\n",
      "[TransformerBlock] After attention and residual connection: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])\n",
      "[FeedForward] Input shape: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM]), Performing feedforward transformations.\n",
      "[FeedForward] Output shape: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])\n",
      "[TransformerBlock] After feedforward and residual connection: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])\n",
      "\n",
      "[TransformerEncoder] Processing layer 7\n",
      "[TransformerBlock] Starting block processing. Input dimensions - value: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM]), key: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM]), query: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])\n",
      "[MultiHeadAttention:1] Input shapes (V, K, Q) - values: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM]), keys: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM]), queries: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])\n",
      "[MultiHeadAttention:2] After linear projections V/K/Q multihead- values: torch.Size([BATCH_SIZE, SEQ_LEN, HEAD_COUNT, EMBEDDING_PER_HEAD]), keys: torch.Size([BATCH_SIZE, SEQ_LEN, HEAD_COUNT, EMBEDDING_PER_HEAD]), queries: torch.Size([BATCH_SIZE, SEQ_LEN, HEAD_COUNT, EMBEDDING_PER_HEAD])\n",
      "[MultiHeadAttention:3] Energy = Q * K (raw attention scores) shape: torch.Size([BATCH_SIZE, HEAD_COUNT, SEQ_LEN, SEQ_LEN])\n",
      "[MultiHeadAttention:4] Applying mask with shape: torch.Size([BATCH_SIZE, 1, 1, SEQ_LEN])\n",
      "[MultiHeadAttention:5] Attention SoftMax weights shape: torch.Size([BATCH_SIZE, HEAD_COUNT, SEQ_LEN, SEQ_LEN])\n",
      "[MultiHeadAttention:6] Attention * V = Output shape after weighted sum: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])\n",
      "[TransformerBlock] After attention and residual connection: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])\n",
      "[FeedForward] Input shape: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM]), Performing feedforward transformations.\n",
      "[FeedForward] Output shape: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])\n",
      "[TransformerBlock] After feedforward and residual connection: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])\n",
      "\n",
      "[TransformerEncoder] Processing layer 8\n",
      "[TransformerBlock] Starting block processing. Input dimensions - value: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM]), key: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM]), query: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])\n",
      "[MultiHeadAttention:1] Input shapes (V, K, Q) - values: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM]), keys: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM]), queries: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])\n",
      "[MultiHeadAttention:2] After linear projections V/K/Q multihead- values: torch.Size([BATCH_SIZE, SEQ_LEN, HEAD_COUNT, EMBEDDING_PER_HEAD]), keys: torch.Size([BATCH_SIZE, SEQ_LEN, HEAD_COUNT, EMBEDDING_PER_HEAD]), queries: torch.Size([BATCH_SIZE, SEQ_LEN, HEAD_COUNT, EMBEDDING_PER_HEAD])\n",
      "[MultiHeadAttention:3] Energy = Q * K (raw attention scores) shape: torch.Size([BATCH_SIZE, HEAD_COUNT, SEQ_LEN, SEQ_LEN])\n",
      "[MultiHeadAttention:4] Applying mask with shape: torch.Size([BATCH_SIZE, 1, 1, SEQ_LEN])\n",
      "[MultiHeadAttention:5] Attention SoftMax weights shape: torch.Size([BATCH_SIZE, HEAD_COUNT, SEQ_LEN, SEQ_LEN])\n",
      "[MultiHeadAttention:6] Attention * V = Output shape after weighted sum: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])\n",
      "[TransformerBlock] After attention and residual connection: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])\n",
      "[FeedForward] Input shape: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM]), Performing feedforward transformations.\n",
      "[FeedForward] Output shape: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])\n",
      "[TransformerBlock] After feedforward and residual connection: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])\n",
      "\n",
      "[TransformerEncoder] Processing layer 9\n",
      "[TransformerBlock] Starting block processing. Input dimensions - value: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM]), key: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM]), query: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])\n",
      "[MultiHeadAttention:1] Input shapes (V, K, Q) - values: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM]), keys: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM]), queries: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])\n",
      "[MultiHeadAttention:2] After linear projections V/K/Q multihead- values: torch.Size([BATCH_SIZE, SEQ_LEN, HEAD_COUNT, EMBEDDING_PER_HEAD]), keys: torch.Size([BATCH_SIZE, SEQ_LEN, HEAD_COUNT, EMBEDDING_PER_HEAD]), queries: torch.Size([BATCH_SIZE, SEQ_LEN, HEAD_COUNT, EMBEDDING_PER_HEAD])\n",
      "[MultiHeadAttention:3] Energy = Q * K (raw attention scores) shape: torch.Size([BATCH_SIZE, HEAD_COUNT, SEQ_LEN, SEQ_LEN])\n",
      "[MultiHeadAttention:4] Applying mask with shape: torch.Size([BATCH_SIZE, 1, 1, SEQ_LEN])\n",
      "[MultiHeadAttention:5] Attention SoftMax weights shape: torch.Size([BATCH_SIZE, HEAD_COUNT, SEQ_LEN, SEQ_LEN])\n",
      "[MultiHeadAttention:6] Attention * V = Output shape after weighted sum: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])\n",
      "[TransformerBlock] After attention and residual connection: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])\n",
      "[FeedForward] Input shape: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM]), Performing feedforward transformations.\n",
      "[FeedForward] Output shape: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])\n",
      "[TransformerBlock] After feedforward and residual connection: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])\n",
      "\n",
      "[TransformerEncoder] Processing layer 10\n",
      "[TransformerBlock] Starting block processing. Input dimensions - value: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM]), key: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM]), query: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])\n",
      "[MultiHeadAttention:1] Input shapes (V, K, Q) - values: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM]), keys: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM]), queries: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])\n",
      "[MultiHeadAttention:2] After linear projections V/K/Q multihead- values: torch.Size([BATCH_SIZE, SEQ_LEN, HEAD_COUNT, EMBEDDING_PER_HEAD]), keys: torch.Size([BATCH_SIZE, SEQ_LEN, HEAD_COUNT, EMBEDDING_PER_HEAD]), queries: torch.Size([BATCH_SIZE, SEQ_LEN, HEAD_COUNT, EMBEDDING_PER_HEAD])\n",
      "[MultiHeadAttention:3] Energy = Q * K (raw attention scores) shape: torch.Size([BATCH_SIZE, HEAD_COUNT, SEQ_LEN, SEQ_LEN])\n",
      "[MultiHeadAttention:4] Applying mask with shape: torch.Size([BATCH_SIZE, 1, 1, SEQ_LEN])\n",
      "[MultiHeadAttention:5] Attention SoftMax weights shape: torch.Size([BATCH_SIZE, HEAD_COUNT, SEQ_LEN, SEQ_LEN])\n",
      "[MultiHeadAttention:6] Attention * V = Output shape after weighted sum: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])\n",
      "[TransformerBlock] After attention and residual connection: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])\n",
      "[FeedForward] Input shape: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM]), Performing feedforward transformations.\n",
      "[FeedForward] Output shape: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])\n",
      "[TransformerBlock] After feedforward and residual connection: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])\n",
      "\n",
      "[TransformerEncoder] Processing layer SEQ_LEN\n",
      "[TransformerBlock] Starting block processing. Input dimensions - value: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM]), key: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM]), query: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])\n",
      "[MultiHeadAttention:1] Input shapes (V, K, Q) - values: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM]), keys: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM]), queries: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])\n",
      "[MultiHeadAttention:2] After linear projections V/K/Q multihead- values: torch.Size([BATCH_SIZE, SEQ_LEN, HEAD_COUNT, EMBEDDING_PER_HEAD]), keys: torch.Size([BATCH_SIZE, SEQ_LEN, HEAD_COUNT, EMBEDDING_PER_HEAD]), queries: torch.Size([BATCH_SIZE, SEQ_LEN, HEAD_COUNT, EMBEDDING_PER_HEAD])\n",
      "[MultiHeadAttention:3] Energy = Q * K (raw attention scores) shape: torch.Size([BATCH_SIZE, HEAD_COUNT, SEQ_LEN, SEQ_LEN])\n",
      "[MultiHeadAttention:4] Applying mask with shape: torch.Size([BATCH_SIZE, 1, 1, SEQ_LEN])\n",
      "[MultiHeadAttention:5] Attention SoftMax weights shape: torch.Size([BATCH_SIZE, HEAD_COUNT, SEQ_LEN, SEQ_LEN])\n",
      "[MultiHeadAttention:6] Attention * V = Output shape after weighted sum: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])\n",
      "[TransformerBlock] After attention and residual connection: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])\n",
      "[FeedForward] Input shape: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM]), Performing feedforward transformations.\n",
      "[FeedForward] Output shape: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])\n",
      "[TransformerBlock] After feedforward and residual connection: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])\n",
      "\n",
      "[TransformerEncoder] Processing layer EMBEDDING_PER_HEAD\n",
      "[TransformerBlock] Starting block processing. Input dimensions - value: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM]), key: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM]), query: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])\n",
      "[MultiHeadAttention:1] Input shapes (V, K, Q) - values: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM]), keys: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM]), queries: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])\n",
      "[MultiHeadAttention:2] After linear projections V/K/Q multihead- values: torch.Size([BATCH_SIZE, SEQ_LEN, HEAD_COUNT, EMBEDDING_PER_HEAD]), keys: torch.Size([BATCH_SIZE, SEQ_LEN, HEAD_COUNT, EMBEDDING_PER_HEAD]), queries: torch.Size([BATCH_SIZE, SEQ_LEN, HEAD_COUNT, EMBEDDING_PER_HEAD])\n",
      "[MultiHeadAttention:3] Energy = Q * K (raw attention scores) shape: torch.Size([BATCH_SIZE, HEAD_COUNT, SEQ_LEN, SEQ_LEN])\n",
      "[MultiHeadAttention:4] Applying mask with shape: torch.Size([BATCH_SIZE, 1, 1, SEQ_LEN])\n",
      "[MultiHeadAttention:5] Attention SoftMax weights shape: torch.Size([BATCH_SIZE, HEAD_COUNT, SEQ_LEN, SEQ_LEN])\n",
      "[MultiHeadAttention:6] Attention * V = Output shape after weighted sum: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])\n",
      "[TransformerBlock] After attention and residual connection: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])\n",
      "[FeedForward] Input shape: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM]), Performing feedforward transformations.\n",
      "[FeedForward] Output shape: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])\n",
      "[TransformerBlock] After feedforward and residual connection: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])\n",
      "[TransformerEncoder] Output x.shape: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])\n",
      "[TransformerDecoder] Input x.shape: torch.Size([BATCH_SIZE, SEQ_LEN]), encoder_out.shape: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])\n",
      "[PositionalEncoding] Input x.shape: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM]), Adding positional encodings for sequence length and embedding size. peshape torch.Size([1, MAX_LEN, EMBEDDING_DIM])\n",
      "while adding, pe[:, :x.size(1)]: torch.Size([1, SEQ_LEN, EMBEDDING_DIM])\n",
      "[PositionalEncoding] Output shape: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])\n",
      "\n",
      "[TransformerDecoder] Processing layer 1\n",
      "[TransformerDecoderBlock] x.shape: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM]), encoder_out.shape: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])\n",
      "[MultiHeadAttention:1] Input shapes (V, K, Q) - values: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM]), keys: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM]), queries: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])\n",
      "[MultiHeadAttention:2] After linear projections V/K/Q multihead- values: torch.Size([BATCH_SIZE, SEQ_LEN, HEAD_COUNT, EMBEDDING_PER_HEAD]), keys: torch.Size([BATCH_SIZE, SEQ_LEN, HEAD_COUNT, EMBEDDING_PER_HEAD]), queries: torch.Size([BATCH_SIZE, SEQ_LEN, HEAD_COUNT, EMBEDDING_PER_HEAD])\n",
      "[MultiHeadAttention:3] Energy = Q * K (raw attention scores) shape: torch.Size([BATCH_SIZE, HEAD_COUNT, SEQ_LEN, SEQ_LEN])\n",
      "[MultiHeadAttention:4] Applying mask with shape: torch.Size([BATCH_SIZE, 1, SEQ_LEN, SEQ_LEN])\n",
      "[MultiHeadAttention:5] Attention SoftMax weights shape: torch.Size([BATCH_SIZE, HEAD_COUNT, SEQ_LEN, SEQ_LEN])\n",
      "[MultiHeadAttention:6] Attention * V = Output shape after weighted sum: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])\n",
      "[MultiHeadAttention:1] Input shapes (V, K, Q) - values: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM]), keys: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM]), queries: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])\n",
      "[MultiHeadAttention:2] After linear projections V/K/Q multihead- values: torch.Size([BATCH_SIZE, SEQ_LEN, HEAD_COUNT, EMBEDDING_PER_HEAD]), keys: torch.Size([BATCH_SIZE, SEQ_LEN, HEAD_COUNT, EMBEDDING_PER_HEAD]), queries: torch.Size([BATCH_SIZE, SEQ_LEN, HEAD_COUNT, EMBEDDING_PER_HEAD])\n",
      "[MultiHeadAttention:3] Energy = Q * K (raw attention scores) shape: torch.Size([BATCH_SIZE, HEAD_COUNT, SEQ_LEN, SEQ_LEN])\n",
      "[MultiHeadAttention:4] Applying mask with shape: torch.Size([BATCH_SIZE, 1, 1, SEQ_LEN])\n",
      "[MultiHeadAttention:5] Attention SoftMax weights shape: torch.Size([BATCH_SIZE, HEAD_COUNT, SEQ_LEN, SEQ_LEN])\n",
      "[MultiHeadAttention:6] Attention * V = Output shape after weighted sum: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])\n",
      "[FeedForward] Input shape: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM]), Performing feedforward transformations.\n",
      "[FeedForward] Output shape: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])\n",
      "[TransformerDecoderBlock] out.shape: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])\n",
      "\n",
      "[TransformerDecoder] Processing layer 2\n",
      "[TransformerDecoderBlock] x.shape: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM]), encoder_out.shape: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])\n",
      "[MultiHeadAttention:1] Input shapes (V, K, Q) - values: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM]), keys: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM]), queries: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])\n",
      "[MultiHeadAttention:2] After linear projections V/K/Q multihead- values: torch.Size([BATCH_SIZE, SEQ_LEN, HEAD_COUNT, EMBEDDING_PER_HEAD]), keys: torch.Size([BATCH_SIZE, SEQ_LEN, HEAD_COUNT, EMBEDDING_PER_HEAD]), queries: torch.Size([BATCH_SIZE, SEQ_LEN, HEAD_COUNT, EMBEDDING_PER_HEAD])\n",
      "[MultiHeadAttention:3] Energy = Q * K (raw attention scores) shape: torch.Size([BATCH_SIZE, HEAD_COUNT, SEQ_LEN, SEQ_LEN])\n",
      "[MultiHeadAttention:4] Applying mask with shape: torch.Size([BATCH_SIZE, 1, SEQ_LEN, SEQ_LEN])\n",
      "[MultiHeadAttention:5] Attention SoftMax weights shape: torch.Size([BATCH_SIZE, HEAD_COUNT, SEQ_LEN, SEQ_LEN])\n",
      "[MultiHeadAttention:6] Attention * V = Output shape after weighted sum: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])\n",
      "[MultiHeadAttention:1] Input shapes (V, K, Q) - values: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM]), keys: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM]), queries: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])\n",
      "[MultiHeadAttention:2] After linear projections V/K/Q multihead- values: torch.Size([BATCH_SIZE, SEQ_LEN, HEAD_COUNT, EMBEDDING_PER_HEAD]), keys: torch.Size([BATCH_SIZE, SEQ_LEN, HEAD_COUNT, EMBEDDING_PER_HEAD]), queries: torch.Size([BATCH_SIZE, SEQ_LEN, HEAD_COUNT, EMBEDDING_PER_HEAD])\n",
      "[MultiHeadAttention:3] Energy = Q * K (raw attention scores) shape: torch.Size([BATCH_SIZE, HEAD_COUNT, SEQ_LEN, SEQ_LEN])\n",
      "[MultiHeadAttention:4] Applying mask with shape: torch.Size([BATCH_SIZE, 1, 1, SEQ_LEN])\n",
      "[MultiHeadAttention:5] Attention SoftMax weights shape: torch.Size([BATCH_SIZE, HEAD_COUNT, SEQ_LEN, SEQ_LEN])\n",
      "[MultiHeadAttention:6] Attention * V = Output shape after weighted sum: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])\n",
      "[FeedForward] Input shape: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM]), Performing feedforward transformations.\n",
      "[FeedForward] Output shape: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])\n",
      "[TransformerDecoderBlock] out.shape: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])\n",
      "\n",
      "[TransformerDecoder] Processing layer 3\n",
      "[TransformerDecoderBlock] x.shape: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM]), encoder_out.shape: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])\n",
      "[MultiHeadAttention:1] Input shapes (V, K, Q) - values: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM]), keys: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM]), queries: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])\n",
      "[MultiHeadAttention:2] After linear projections V/K/Q multihead- values: torch.Size([BATCH_SIZE, SEQ_LEN, HEAD_COUNT, EMBEDDING_PER_HEAD]), keys: torch.Size([BATCH_SIZE, SEQ_LEN, HEAD_COUNT, EMBEDDING_PER_HEAD]), queries: torch.Size([BATCH_SIZE, SEQ_LEN, HEAD_COUNT, EMBEDDING_PER_HEAD])\n",
      "[MultiHeadAttention:3] Energy = Q * K (raw attention scores) shape: torch.Size([BATCH_SIZE, HEAD_COUNT, SEQ_LEN, SEQ_LEN])\n",
      "[MultiHeadAttention:4] Applying mask with shape: torch.Size([BATCH_SIZE, 1, SEQ_LEN, SEQ_LEN])\n",
      "[MultiHeadAttention:5] Attention SoftMax weights shape: torch.Size([BATCH_SIZE, HEAD_COUNT, SEQ_LEN, SEQ_LEN])\n",
      "[MultiHeadAttention:6] Attention * V = Output shape after weighted sum: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])\n",
      "[MultiHeadAttention:1] Input shapes (V, K, Q) - values: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM]), keys: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM]), queries: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])\n",
      "[MultiHeadAttention:2] After linear projections V/K/Q multihead- values: torch.Size([BATCH_SIZE, SEQ_LEN, HEAD_COUNT, EMBEDDING_PER_HEAD]), keys: torch.Size([BATCH_SIZE, SEQ_LEN, HEAD_COUNT, EMBEDDING_PER_HEAD]), queries: torch.Size([BATCH_SIZE, SEQ_LEN, HEAD_COUNT, EMBEDDING_PER_HEAD])\n",
      "[MultiHeadAttention:3] Energy = Q * K (raw attention scores) shape: torch.Size([BATCH_SIZE, HEAD_COUNT, SEQ_LEN, SEQ_LEN])\n",
      "[MultiHeadAttention:4] Applying mask with shape: torch.Size([BATCH_SIZE, 1, 1, SEQ_LEN])\n",
      "[MultiHeadAttention:5] Attention SoftMax weights shape: torch.Size([BATCH_SIZE, HEAD_COUNT, SEQ_LEN, SEQ_LEN])\n",
      "[MultiHeadAttention:6] Attention * V = Output shape after weighted sum: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])\n",
      "[FeedForward] Input shape: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM]), Performing feedforward transformations.\n",
      "[FeedForward] Output shape: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])\n",
      "[TransformerDecoderBlock] out.shape: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])\n",
      "\n",
      "[TransformerDecoder] Processing layer 4\n",
      "[TransformerDecoderBlock] x.shape: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM]), encoder_out.shape: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])\n",
      "[MultiHeadAttention:1] Input shapes (V, K, Q) - values: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM]), keys: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM]), queries: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])\n",
      "[MultiHeadAttention:2] After linear projections V/K/Q multihead- values: torch.Size([BATCH_SIZE, SEQ_LEN, HEAD_COUNT, EMBEDDING_PER_HEAD]), keys: torch.Size([BATCH_SIZE, SEQ_LEN, HEAD_COUNT, EMBEDDING_PER_HEAD]), queries: torch.Size([BATCH_SIZE, SEQ_LEN, HEAD_COUNT, EMBEDDING_PER_HEAD])\n",
      "[MultiHeadAttention:3] Energy = Q * K (raw attention scores) shape: torch.Size([BATCH_SIZE, HEAD_COUNT, SEQ_LEN, SEQ_LEN])\n",
      "[MultiHeadAttention:4] Applying mask with shape: torch.Size([BATCH_SIZE, 1, SEQ_LEN, SEQ_LEN])\n",
      "[MultiHeadAttention:5] Attention SoftMax weights shape: torch.Size([BATCH_SIZE, HEAD_COUNT, SEQ_LEN, SEQ_LEN])\n",
      "[MultiHeadAttention:6] Attention * V = Output shape after weighted sum: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])\n",
      "[MultiHeadAttention:1] Input shapes (V, K, Q) - values: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM]), keys: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM]), queries: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])\n",
      "[MultiHeadAttention:2] After linear projections V/K/Q multihead- values: torch.Size([BATCH_SIZE, SEQ_LEN, HEAD_COUNT, EMBEDDING_PER_HEAD]), keys: torch.Size([BATCH_SIZE, SEQ_LEN, HEAD_COUNT, EMBEDDING_PER_HEAD]), queries: torch.Size([BATCH_SIZE, SEQ_LEN, HEAD_COUNT, EMBEDDING_PER_HEAD])\n",
      "[MultiHeadAttention:3] Energy = Q * K (raw attention scores) shape: torch.Size([BATCH_SIZE, HEAD_COUNT, SEQ_LEN, SEQ_LEN])\n",
      "[MultiHeadAttention:4] Applying mask with shape: torch.Size([BATCH_SIZE, 1, 1, SEQ_LEN])\n",
      "[MultiHeadAttention:5] Attention SoftMax weights shape: torch.Size([BATCH_SIZE, HEAD_COUNT, SEQ_LEN, SEQ_LEN])\n",
      "[MultiHeadAttention:6] Attention * V = Output shape after weighted sum: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])\n",
      "[FeedForward] Input shape: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM]), Performing feedforward transformations.\n",
      "[FeedForward] Output shape: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])\n",
      "[TransformerDecoderBlock] out.shape: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])\n",
      "\n",
      "[TransformerDecoder] Processing layer 5\n",
      "[TransformerDecoderBlock] x.shape: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM]), encoder_out.shape: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])\n",
      "[MultiHeadAttention:1] Input shapes (V, K, Q) - values: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM]), keys: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM]), queries: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])\n",
      "[MultiHeadAttention:2] After linear projections V/K/Q multihead- values: torch.Size([BATCH_SIZE, SEQ_LEN, HEAD_COUNT, EMBEDDING_PER_HEAD]), keys: torch.Size([BATCH_SIZE, SEQ_LEN, HEAD_COUNT, EMBEDDING_PER_HEAD]), queries: torch.Size([BATCH_SIZE, SEQ_LEN, HEAD_COUNT, EMBEDDING_PER_HEAD])\n",
      "[MultiHeadAttention:3] Energy = Q * K (raw attention scores) shape: torch.Size([BATCH_SIZE, HEAD_COUNT, SEQ_LEN, SEQ_LEN])\n",
      "[MultiHeadAttention:4] Applying mask with shape: torch.Size([BATCH_SIZE, 1, SEQ_LEN, SEQ_LEN])\n",
      "[MultiHeadAttention:5] Attention SoftMax weights shape: torch.Size([BATCH_SIZE, HEAD_COUNT, SEQ_LEN, SEQ_LEN])\n",
      "[MultiHeadAttention:6] Attention * V = Output shape after weighted sum: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])\n",
      "[MultiHeadAttention:1] Input shapes (V, K, Q) - values: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM]), keys: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM]), queries: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])\n",
      "[MultiHeadAttention:2] After linear projections V/K/Q multihead- values: torch.Size([BATCH_SIZE, SEQ_LEN, HEAD_COUNT, EMBEDDING_PER_HEAD]), keys: torch.Size([BATCH_SIZE, SEQ_LEN, HEAD_COUNT, EMBEDDING_PER_HEAD]), queries: torch.Size([BATCH_SIZE, SEQ_LEN, HEAD_COUNT, EMBEDDING_PER_HEAD])\n",
      "[MultiHeadAttention:3] Energy = Q * K (raw attention scores) shape: torch.Size([BATCH_SIZE, HEAD_COUNT, SEQ_LEN, SEQ_LEN])\n",
      "[MultiHeadAttention:4] Applying mask with shape: torch.Size([BATCH_SIZE, 1, 1, SEQ_LEN])\n",
      "[MultiHeadAttention:5] Attention SoftMax weights shape: torch.Size([BATCH_SIZE, HEAD_COUNT, SEQ_LEN, SEQ_LEN])\n",
      "[MultiHeadAttention:6] Attention * V = Output shape after weighted sum: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])\n",
      "[FeedForward] Input shape: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM]), Performing feedforward transformations.\n",
      "[FeedForward] Output shape: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])\n",
      "[TransformerDecoderBlock] out.shape: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])\n",
      "\n",
      "[TransformerDecoder] Processing layer 6\n",
      "[TransformerDecoderBlock] x.shape: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM]), encoder_out.shape: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])\n",
      "[MultiHeadAttention:1] Input shapes (V, K, Q) - values: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM]), keys: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM]), queries: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])\n",
      "[MultiHeadAttention:2] After linear projections V/K/Q multihead- values: torch.Size([BATCH_SIZE, SEQ_LEN, HEAD_COUNT, EMBEDDING_PER_HEAD]), keys: torch.Size([BATCH_SIZE, SEQ_LEN, HEAD_COUNT, EMBEDDING_PER_HEAD]), queries: torch.Size([BATCH_SIZE, SEQ_LEN, HEAD_COUNT, EMBEDDING_PER_HEAD])\n",
      "[MultiHeadAttention:3] Energy = Q * K (raw attention scores) shape: torch.Size([BATCH_SIZE, HEAD_COUNT, SEQ_LEN, SEQ_LEN])\n",
      "[MultiHeadAttention:4] Applying mask with shape: torch.Size([BATCH_SIZE, 1, SEQ_LEN, SEQ_LEN])\n",
      "[MultiHeadAttention:5] Attention SoftMax weights shape: torch.Size([BATCH_SIZE, HEAD_COUNT, SEQ_LEN, SEQ_LEN])\n",
      "[MultiHeadAttention:6] Attention * V = Output shape after weighted sum: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])\n",
      "[MultiHeadAttention:1] Input shapes (V, K, Q) - values: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM]), keys: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM]), queries: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])\n",
      "[MultiHeadAttention:2] After linear projections V/K/Q multihead- values: torch.Size([BATCH_SIZE, SEQ_LEN, HEAD_COUNT, EMBEDDING_PER_HEAD]), keys: torch.Size([BATCH_SIZE, SEQ_LEN, HEAD_COUNT, EMBEDDING_PER_HEAD]), queries: torch.Size([BATCH_SIZE, SEQ_LEN, HEAD_COUNT, EMBEDDING_PER_HEAD])\n",
      "[MultiHeadAttention:3] Energy = Q * K (raw attention scores) shape: torch.Size([BATCH_SIZE, HEAD_COUNT, SEQ_LEN, SEQ_LEN])\n",
      "[MultiHeadAttention:4] Applying mask with shape: torch.Size([BATCH_SIZE, 1, 1, SEQ_LEN])\n",
      "[MultiHeadAttention:5] Attention SoftMax weights shape: torch.Size([BATCH_SIZE, HEAD_COUNT, SEQ_LEN, SEQ_LEN])\n",
      "[MultiHeadAttention:6] Attention * V = Output shape after weighted sum: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])\n",
      "[FeedForward] Input shape: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM]), Performing feedforward transformations.\n",
      "[FeedForward] Output shape: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])\n",
      "[TransformerDecoderBlock] out.shape: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])\n",
      "\n",
      "[TransformerDecoder] Processing layer 7\n",
      "[TransformerDecoderBlock] x.shape: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM]), encoder_out.shape: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])\n",
      "[MultiHeadAttention:1] Input shapes (V, K, Q) - values: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM]), keys: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM]), queries: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])\n",
      "[MultiHeadAttention:2] After linear projections V/K/Q multihead- values: torch.Size([BATCH_SIZE, SEQ_LEN, HEAD_COUNT, EMBEDDING_PER_HEAD]), keys: torch.Size([BATCH_SIZE, SEQ_LEN, HEAD_COUNT, EMBEDDING_PER_HEAD]), queries: torch.Size([BATCH_SIZE, SEQ_LEN, HEAD_COUNT, EMBEDDING_PER_HEAD])\n",
      "[MultiHeadAttention:3] Energy = Q * K (raw attention scores) shape: torch.Size([BATCH_SIZE, HEAD_COUNT, SEQ_LEN, SEQ_LEN])\n",
      "[MultiHeadAttention:4] Applying mask with shape: torch.Size([BATCH_SIZE, 1, SEQ_LEN, SEQ_LEN])\n",
      "[MultiHeadAttention:5] Attention SoftMax weights shape: torch.Size([BATCH_SIZE, HEAD_COUNT, SEQ_LEN, SEQ_LEN])\n",
      "[MultiHeadAttention:6] Attention * V = Output shape after weighted sum: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])\n",
      "[MultiHeadAttention:1] Input shapes (V, K, Q) - values: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM]), keys: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM]), queries: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])\n",
      "[MultiHeadAttention:2] After linear projections V/K/Q multihead- values: torch.Size([BATCH_SIZE, SEQ_LEN, HEAD_COUNT, EMBEDDING_PER_HEAD]), keys: torch.Size([BATCH_SIZE, SEQ_LEN, HEAD_COUNT, EMBEDDING_PER_HEAD]), queries: torch.Size([BATCH_SIZE, SEQ_LEN, HEAD_COUNT, EMBEDDING_PER_HEAD])\n",
      "[MultiHeadAttention:3] Energy = Q * K (raw attention scores) shape: torch.Size([BATCH_SIZE, HEAD_COUNT, SEQ_LEN, SEQ_LEN])\n",
      "[MultiHeadAttention:4] Applying mask with shape: torch.Size([BATCH_SIZE, 1, 1, SEQ_LEN])\n",
      "[MultiHeadAttention:5] Attention SoftMax weights shape: torch.Size([BATCH_SIZE, HEAD_COUNT, SEQ_LEN, SEQ_LEN])\n",
      "[MultiHeadAttention:6] Attention * V = Output shape after weighted sum: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])\n",
      "[FeedForward] Input shape: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM]), Performing feedforward transformations.\n",
      "[FeedForward] Output shape: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])\n",
      "[TransformerDecoderBlock] out.shape: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])\n",
      "\n",
      "[TransformerDecoder] Processing layer 8\n",
      "[TransformerDecoderBlock] x.shape: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM]), encoder_out.shape: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])\n",
      "[MultiHeadAttention:1] Input shapes (V, K, Q) - values: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM]), keys: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM]), queries: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])\n",
      "[MultiHeadAttention:2] After linear projections V/K/Q multihead- values: torch.Size([BATCH_SIZE, SEQ_LEN, HEAD_COUNT, EMBEDDING_PER_HEAD]), keys: torch.Size([BATCH_SIZE, SEQ_LEN, HEAD_COUNT, EMBEDDING_PER_HEAD]), queries: torch.Size([BATCH_SIZE, SEQ_LEN, HEAD_COUNT, EMBEDDING_PER_HEAD])\n",
      "[MultiHeadAttention:3] Energy = Q * K (raw attention scores) shape: torch.Size([BATCH_SIZE, HEAD_COUNT, SEQ_LEN, SEQ_LEN])\n",
      "[MultiHeadAttention:4] Applying mask with shape: torch.Size([BATCH_SIZE, 1, SEQ_LEN, SEQ_LEN])\n",
      "[MultiHeadAttention:5] Attention SoftMax weights shape: torch.Size([BATCH_SIZE, HEAD_COUNT, SEQ_LEN, SEQ_LEN])\n",
      "[MultiHeadAttention:6] Attention * V = Output shape after weighted sum: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])\n",
      "[MultiHeadAttention:1] Input shapes (V, K, Q) - values: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM]), keys: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM]), queries: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])\n",
      "[MultiHeadAttention:2] After linear projections V/K/Q multihead- values: torch.Size([BATCH_SIZE, SEQ_LEN, HEAD_COUNT, EMBEDDING_PER_HEAD]), keys: torch.Size([BATCH_SIZE, SEQ_LEN, HEAD_COUNT, EMBEDDING_PER_HEAD]), queries: torch.Size([BATCH_SIZE, SEQ_LEN, HEAD_COUNT, EMBEDDING_PER_HEAD])\n",
      "[MultiHeadAttention:3] Energy = Q * K (raw attention scores) shape: torch.Size([BATCH_SIZE, HEAD_COUNT, SEQ_LEN, SEQ_LEN])\n",
      "[MultiHeadAttention:4] Applying mask with shape: torch.Size([BATCH_SIZE, 1, 1, SEQ_LEN])\n",
      "[MultiHeadAttention:5] Attention SoftMax weights shape: torch.Size([BATCH_SIZE, HEAD_COUNT, SEQ_LEN, SEQ_LEN])\n",
      "[MultiHeadAttention:6] Attention * V = Output shape after weighted sum: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])\n",
      "[FeedForward] Input shape: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM]), Performing feedforward transformations.\n",
      "[FeedForward] Output shape: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])\n",
      "[TransformerDecoderBlock] out.shape: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])\n",
      "\n",
      "[TransformerDecoder] Processing layer 9\n",
      "[TransformerDecoderBlock] x.shape: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM]), encoder_out.shape: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])\n",
      "[MultiHeadAttention:1] Input shapes (V, K, Q) - values: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM]), keys: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM]), queries: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])\n",
      "[MultiHeadAttention:2] After linear projections V/K/Q multihead- values: torch.Size([BATCH_SIZE, SEQ_LEN, HEAD_COUNT, EMBEDDING_PER_HEAD]), keys: torch.Size([BATCH_SIZE, SEQ_LEN, HEAD_COUNT, EMBEDDING_PER_HEAD]), queries: torch.Size([BATCH_SIZE, SEQ_LEN, HEAD_COUNT, EMBEDDING_PER_HEAD])\n",
      "[MultiHeadAttention:3] Energy = Q * K (raw attention scores) shape: torch.Size([BATCH_SIZE, HEAD_COUNT, SEQ_LEN, SEQ_LEN])\n",
      "[MultiHeadAttention:4] Applying mask with shape: torch.Size([BATCH_SIZE, 1, SEQ_LEN, SEQ_LEN])\n",
      "[MultiHeadAttention:5] Attention SoftMax weights shape: torch.Size([BATCH_SIZE, HEAD_COUNT, SEQ_LEN, SEQ_LEN])\n",
      "[MultiHeadAttention:6] Attention * V = Output shape after weighted sum: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])\n",
      "[MultiHeadAttention:1] Input shapes (V, K, Q) - values: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM]), keys: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM]), queries: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])\n",
      "[MultiHeadAttention:2] After linear projections V/K/Q multihead- values: torch.Size([BATCH_SIZE, SEQ_LEN, HEAD_COUNT, EMBEDDING_PER_HEAD]), keys: torch.Size([BATCH_SIZE, SEQ_LEN, HEAD_COUNT, EMBEDDING_PER_HEAD]), queries: torch.Size([BATCH_SIZE, SEQ_LEN, HEAD_COUNT, EMBEDDING_PER_HEAD])\n",
      "[MultiHeadAttention:3] Energy = Q * K (raw attention scores) shape: torch.Size([BATCH_SIZE, HEAD_COUNT, SEQ_LEN, SEQ_LEN])\n",
      "[MultiHeadAttention:4] Applying mask with shape: torch.Size([BATCH_SIZE, 1, 1, SEQ_LEN])\n",
      "[MultiHeadAttention:5] Attention SoftMax weights shape: torch.Size([BATCH_SIZE, HEAD_COUNT, SEQ_LEN, SEQ_LEN])\n",
      "[MultiHeadAttention:6] Attention * V = Output shape after weighted sum: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])\n",
      "[FeedForward] Input shape: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM]), Performing feedforward transformations.\n",
      "[FeedForward] Output shape: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])\n",
      "[TransformerDecoderBlock] out.shape: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])\n",
      "\n",
      "[TransformerDecoder] Processing layer 10\n",
      "[TransformerDecoderBlock] x.shape: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM]), encoder_out.shape: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])\n",
      "[MultiHeadAttention:1] Input shapes (V, K, Q) - values: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM]), keys: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM]), queries: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])\n",
      "[MultiHeadAttention:2] After linear projections V/K/Q multihead- values: torch.Size([BATCH_SIZE, SEQ_LEN, HEAD_COUNT, EMBEDDING_PER_HEAD]), keys: torch.Size([BATCH_SIZE, SEQ_LEN, HEAD_COUNT, EMBEDDING_PER_HEAD]), queries: torch.Size([BATCH_SIZE, SEQ_LEN, HEAD_COUNT, EMBEDDING_PER_HEAD])\n",
      "[MultiHeadAttention:3] Energy = Q * K (raw attention scores) shape: torch.Size([BATCH_SIZE, HEAD_COUNT, SEQ_LEN, SEQ_LEN])\n",
      "[MultiHeadAttention:4] Applying mask with shape: torch.Size([BATCH_SIZE, 1, SEQ_LEN, SEQ_LEN])\n",
      "[MultiHeadAttention:5] Attention SoftMax weights shape: torch.Size([BATCH_SIZE, HEAD_COUNT, SEQ_LEN, SEQ_LEN])\n",
      "[MultiHeadAttention:6] Attention * V = Output shape after weighted sum: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])\n",
      "[MultiHeadAttention:1] Input shapes (V, K, Q) - values: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM]), keys: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM]), queries: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])\n",
      "[MultiHeadAttention:2] After linear projections V/K/Q multihead- values: torch.Size([BATCH_SIZE, SEQ_LEN, HEAD_COUNT, EMBEDDING_PER_HEAD]), keys: torch.Size([BATCH_SIZE, SEQ_LEN, HEAD_COUNT, EMBEDDING_PER_HEAD]), queries: torch.Size([BATCH_SIZE, SEQ_LEN, HEAD_COUNT, EMBEDDING_PER_HEAD])\n",
      "[MultiHeadAttention:3] Energy = Q * K (raw attention scores) shape: torch.Size([BATCH_SIZE, HEAD_COUNT, SEQ_LEN, SEQ_LEN])\n",
      "[MultiHeadAttention:4] Applying mask with shape: torch.Size([BATCH_SIZE, 1, 1, SEQ_LEN])\n",
      "[MultiHeadAttention:5] Attention SoftMax weights shape: torch.Size([BATCH_SIZE, HEAD_COUNT, SEQ_LEN, SEQ_LEN])\n",
      "[MultiHeadAttention:6] Attention * V = Output shape after weighted sum: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])\n",
      "[FeedForward] Input shape: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM]), Performing feedforward transformations.\n",
      "[FeedForward] Output shape: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])\n",
      "[TransformerDecoderBlock] out.shape: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])\n",
      "\n",
      "[TransformerDecoder] Processing layer SEQ_LEN\n",
      "[TransformerDecoderBlock] x.shape: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM]), encoder_out.shape: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])\n",
      "[MultiHeadAttention:1] Input shapes (V, K, Q) - values: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM]), keys: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM]), queries: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])\n",
      "[MultiHeadAttention:2] After linear projections V/K/Q multihead- values: torch.Size([BATCH_SIZE, SEQ_LEN, HEAD_COUNT, EMBEDDING_PER_HEAD]), keys: torch.Size([BATCH_SIZE, SEQ_LEN, HEAD_COUNT, EMBEDDING_PER_HEAD]), queries: torch.Size([BATCH_SIZE, SEQ_LEN, HEAD_COUNT, EMBEDDING_PER_HEAD])\n",
      "[MultiHeadAttention:3] Energy = Q * K (raw attention scores) shape: torch.Size([BATCH_SIZE, HEAD_COUNT, SEQ_LEN, SEQ_LEN])\n",
      "[MultiHeadAttention:4] Applying mask with shape: torch.Size([BATCH_SIZE, 1, SEQ_LEN, SEQ_LEN])\n",
      "[MultiHeadAttention:5] Attention SoftMax weights shape: torch.Size([BATCH_SIZE, HEAD_COUNT, SEQ_LEN, SEQ_LEN])\n",
      "[MultiHeadAttention:6] Attention * V = Output shape after weighted sum: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])\n",
      "[MultiHeadAttention:1] Input shapes (V, K, Q) - values: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM]), keys: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM]), queries: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])\n",
      "[MultiHeadAttention:2] After linear projections V/K/Q multihead- values: torch.Size([BATCH_SIZE, SEQ_LEN, HEAD_COUNT, EMBEDDING_PER_HEAD]), keys: torch.Size([BATCH_SIZE, SEQ_LEN, HEAD_COUNT, EMBEDDING_PER_HEAD]), queries: torch.Size([BATCH_SIZE, SEQ_LEN, HEAD_COUNT, EMBEDDING_PER_HEAD])\n",
      "[MultiHeadAttention:3] Energy = Q * K (raw attention scores) shape: torch.Size([BATCH_SIZE, HEAD_COUNT, SEQ_LEN, SEQ_LEN])\n",
      "[MultiHeadAttention:4] Applying mask with shape: torch.Size([BATCH_SIZE, 1, 1, SEQ_LEN])\n",
      "[MultiHeadAttention:5] Attention SoftMax weights shape: torch.Size([BATCH_SIZE, HEAD_COUNT, SEQ_LEN, SEQ_LEN])\n",
      "[MultiHeadAttention:6] Attention * V = Output shape after weighted sum: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])\n",
      "[FeedForward] Input shape: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM]), Performing feedforward transformations.\n",
      "[FeedForward] Output shape: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])\n",
      "[TransformerDecoderBlock] out.shape: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])\n",
      "\n",
      "[TransformerDecoder] Processing layer EMBEDDING_PER_HEAD\n",
      "[TransformerDecoderBlock] x.shape: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM]), encoder_out.shape: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])\n",
      "[MultiHeadAttention:1] Input shapes (V, K, Q) - values: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM]), keys: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM]), queries: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])\n",
      "[MultiHeadAttention:2] After linear projections V/K/Q multihead- values: torch.Size([BATCH_SIZE, SEQ_LEN, HEAD_COUNT, EMBEDDING_PER_HEAD]), keys: torch.Size([BATCH_SIZE, SEQ_LEN, HEAD_COUNT, EMBEDDING_PER_HEAD]), queries: torch.Size([BATCH_SIZE, SEQ_LEN, HEAD_COUNT, EMBEDDING_PER_HEAD])\n",
      "[MultiHeadAttention:3] Energy = Q * K (raw attention scores) shape: torch.Size([BATCH_SIZE, HEAD_COUNT, SEQ_LEN, SEQ_LEN])\n",
      "[MultiHeadAttention:4] Applying mask with shape: torch.Size([BATCH_SIZE, 1, SEQ_LEN, SEQ_LEN])\n",
      "[MultiHeadAttention:5] Attention SoftMax weights shape: torch.Size([BATCH_SIZE, HEAD_COUNT, SEQ_LEN, SEQ_LEN])\n",
      "[MultiHeadAttention:6] Attention * V = Output shape after weighted sum: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])\n",
      "[MultiHeadAttention:1] Input shapes (V, K, Q) - values: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM]), keys: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM]), queries: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])\n",
      "[MultiHeadAttention:2] After linear projections V/K/Q multihead- values: torch.Size([BATCH_SIZE, SEQ_LEN, HEAD_COUNT, EMBEDDING_PER_HEAD]), keys: torch.Size([BATCH_SIZE, SEQ_LEN, HEAD_COUNT, EMBEDDING_PER_HEAD]), queries: torch.Size([BATCH_SIZE, SEQ_LEN, HEAD_COUNT, EMBEDDING_PER_HEAD])\n",
      "[MultiHeadAttention:3] Energy = Q * K (raw attention scores) shape: torch.Size([BATCH_SIZE, HEAD_COUNT, SEQ_LEN, SEQ_LEN])\n",
      "[MultiHeadAttention:4] Applying mask with shape: torch.Size([BATCH_SIZE, 1, 1, SEQ_LEN])\n",
      "[MultiHeadAttention:5] Attention SoftMax weights shape: torch.Size([BATCH_SIZE, HEAD_COUNT, SEQ_LEN, SEQ_LEN])\n",
      "[MultiHeadAttention:6] Attention * V = Output shape after weighted sum: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])\n",
      "[FeedForward] Input shape: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM]), Performing feedforward transformations.\n",
      "[FeedForward] Output shape: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])\n",
      "[TransformerDecoderBlock] out.shape: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])\n",
      "[TransformerDecoder] Output x.shape: torch.Size([BATCH_SIZE, SEQ_LEN, VOCAB_SIZE])\n",
      "[Main] Final output shape: torch.Size([BATCH_SIZE, SEQ_LEN, VOCAB_SIZE])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def parse_logs(logs):\n",
    "    replacements = {\n",
    "        \"100\": \"MAX_LEN\",\n",
    "        \"384\": \"EMBEDDING_DIM\",\n",
    "        \"16\": \"BATCH_SIZE\",\n",
    "        \"11\": \"SEQ_LEN\",\n",
    "        \"32\": \"HEAD_COUNT\",\n",
    "        \"12\": \"EMBEDDING_PER_HEAD\",\n",
    "        \"64\": \"FF_DIM\",\n",
    "        \"0.1\": \"DROPOUT_RATE\",\n",
    "        \"256\": \"VOCAB_SIZE\",\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "    parsed_logs = logs\n",
    "    for original, replacement in replacements.items():\n",
    "        parsed_logs = parsed_logs.replace(original, replacement)\n",
    "\n",
    "    return parsed_logs\n",
    "\n",
    "# Example log input\n",
    "logs = open(\"./log.txt\").read()\n",
    "# Parse the logs\n",
    "parsed_logs = parse_logs(logs)\n",
    "print(parsed_logs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
