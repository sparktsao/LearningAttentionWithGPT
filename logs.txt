position encoder pe shape torch.Size([MAX_LEN, EMBEDDING_DIM])
position encoder pe shape torch.Size([MAX_LEN, EMBEDDING_DIM])
[Transformer] src.shape: torch.Size([BATCH_SIZE, SEQ_LEN]), trg.shape: torch.Size([BATCH_SIZE, SEQ_LEN])
[make_src_mask] mask.shape: torch.Size([BATCH_SIZE, 1, 1, SEQ_LEN])
[make_trg_mask] mask.shape: torch.Size([BATCH_SIZE, 1, SEQ_LEN, SEQ_LEN])
[TransformerEncoder] Input x.shape: torch.Size([BATCH_SIZE, SEQ_LEN])
[TransformerEncoder] After word embedding x.shape: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])
[PositionalEncoding] Input x.shape: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM]), Adding positional encodings for sequence length and embedding size. peshape torch.Size([1, MAX_LEN, EMBEDDING_DIM])
while adding, pe[:, :x.size(1)]: torch.Size([1, SEQ_LEN, EMBEDDING_DIM])
[PositionalEncoding] Output shape: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])

[TransformerEncoder] Processing layer 1
[TransformerBlock] Starting block processing. Input dimensions - value: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM]), key: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM]), query: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])
[MultiHeadAttention:1] Input shapes (V, K, Q) - values: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM]), keys: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM]), queries: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])
[MultiHeadAttention:2] After linear projections V/K/Q multihead- values: torch.Size([BATCH_SIZE, SEQ_LEN, HEAD_COUNT, EMBEDDING_PER_HEAD]), keys: torch.Size([BATCH_SIZE, SEQ_LEN, HEAD_COUNT, EMBEDDING_PER_HEAD]), queries: torch.Size([BATCH_SIZE, SEQ_LEN, HEAD_COUNT, EMBEDDING_PER_HEAD])
[MultiHeadAttention:3] Energy = Q * K (raw attention scores) shape: torch.Size([BATCH_SIZE, HEAD_COUNT, SEQ_LEN, SEQ_LEN])
[MultiHeadAttention:4] Applying mask with shape: torch.Size([BATCH_SIZE, 1, 1, SEQ_LEN])
[MultiHeadAttention:5] Attention SoftMax weights shape: torch.Size([BATCH_SIZE, HEAD_COUNT, SEQ_LEN, SEQ_LEN])
[MultiHeadAttention:6] Attention * V = Output shape after weighted sum: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])
[TransformerBlock] After attention and residual connection: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])
[FeedForward] Input shape: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM]), Performing feedforward transformations.
[FeedForward] Output shape: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])
[TransformerBlock] After feedforward and residual connection: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])

[TransformerEncoder] Processing layer 2
[TransformerBlock] Starting block processing. Input dimensions - value: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM]), key: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM]), query: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])
[MultiHeadAttention:1] Input shapes (V, K, Q) - values: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM]), keys: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM]), queries: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])
[MultiHeadAttention:2] After linear projections V/K/Q multihead- values: torch.Size([BATCH_SIZE, SEQ_LEN, HEAD_COUNT, EMBEDDING_PER_HEAD]), keys: torch.Size([BATCH_SIZE, SEQ_LEN, HEAD_COUNT, EMBEDDING_PER_HEAD]), queries: torch.Size([BATCH_SIZE, SEQ_LEN, HEAD_COUNT, EMBEDDING_PER_HEAD])
[MultiHeadAttention:3] Energy = Q * K (raw attention scores) shape: torch.Size([BATCH_SIZE, HEAD_COUNT, SEQ_LEN, SEQ_LEN])
[MultiHeadAttention:4] Applying mask with shape: torch.Size([BATCH_SIZE, 1, 1, SEQ_LEN])
[MultiHeadAttention:5] Attention SoftMax weights shape: torch.Size([BATCH_SIZE, HEAD_COUNT, SEQ_LEN, SEQ_LEN])
[MultiHeadAttention:6] Attention * V = Output shape after weighted sum: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])
[TransformerBlock] After attention and residual connection: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])
[FeedForward] Input shape: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM]), Performing feedforward transformations.
[FeedForward] Output shape: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])
[TransformerBlock] After feedforward and residual connection: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])

[TransformerEncoder] Processing layer 3
[TransformerBlock] Starting block processing. Input dimensions - value: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM]), key: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM]), query: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])
[MultiHeadAttention:1] Input shapes (V, K, Q) - values: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM]), keys: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM]), queries: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])
[MultiHeadAttention:2] After linear projections V/K/Q multihead- values: torch.Size([BATCH_SIZE, SEQ_LEN, HEAD_COUNT, EMBEDDING_PER_HEAD]), keys: torch.Size([BATCH_SIZE, SEQ_LEN, HEAD_COUNT, EMBEDDING_PER_HEAD]), queries: torch.Size([BATCH_SIZE, SEQ_LEN, HEAD_COUNT, EMBEDDING_PER_HEAD])
[MultiHeadAttention:3] Energy = Q * K (raw attention scores) shape: torch.Size([BATCH_SIZE, HEAD_COUNT, SEQ_LEN, SEQ_LEN])
[MultiHeadAttention:4] Applying mask with shape: torch.Size([BATCH_SIZE, 1, 1, SEQ_LEN])
[MultiHeadAttention:5] Attention SoftMax weights shape: torch.Size([BATCH_SIZE, HEAD_COUNT, SEQ_LEN, SEQ_LEN])
[MultiHeadAttention:6] Attention * V = Output shape after weighted sum: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])
[TransformerBlock] After attention and residual connection: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])
[FeedForward] Input shape: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM]), Performing feedforward transformations.
[FeedForward] Output shape: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])
[TransformerBlock] After feedforward and residual connection: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])

[TransformerEncoder] Processing layer 4
[TransformerBlock] Starting block processing. Input dimensions - value: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM]), key: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM]), query: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])
[MultiHeadAttention:1] Input shapes (V, K, Q) - values: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM]), keys: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM]), queries: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])
[MultiHeadAttention:2] After linear projections V/K/Q multihead- values: torch.Size([BATCH_SIZE, SEQ_LEN, HEAD_COUNT, EMBEDDING_PER_HEAD]), keys: torch.Size([BATCH_SIZE, SEQ_LEN, HEAD_COUNT, EMBEDDING_PER_HEAD]), queries: torch.Size([BATCH_SIZE, SEQ_LEN, HEAD_COUNT, EMBEDDING_PER_HEAD])
[MultiHeadAttention:3] Energy = Q * K (raw attention scores) shape: torch.Size([BATCH_SIZE, HEAD_COUNT, SEQ_LEN, SEQ_LEN])
[MultiHeadAttention:4] Applying mask with shape: torch.Size([BATCH_SIZE, 1, 1, SEQ_LEN])
[MultiHeadAttention:5] Attention SoftMax weights shape: torch.Size([BATCH_SIZE, HEAD_COUNT, SEQ_LEN, SEQ_LEN])
[MultiHeadAttention:6] Attention * V = Output shape after weighted sum: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])
[TransformerBlock] After attention and residual connection: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])
[FeedForward] Input shape: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM]), Performing feedforward transformations.
[FeedForward] Output shape: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])
[TransformerBlock] After feedforward and residual connection: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])

[TransformerEncoder] Processing layer 5
[TransformerBlock] Starting block processing. Input dimensions - value: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM]), key: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM]), query: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])
[MultiHeadAttention:1] Input shapes (V, K, Q) - values: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM]), keys: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM]), queries: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])
[MultiHeadAttention:2] After linear projections V/K/Q multihead- values: torch.Size([BATCH_SIZE, SEQ_LEN, HEAD_COUNT, EMBEDDING_PER_HEAD]), keys: torch.Size([BATCH_SIZE, SEQ_LEN, HEAD_COUNT, EMBEDDING_PER_HEAD]), queries: torch.Size([BATCH_SIZE, SEQ_LEN, HEAD_COUNT, EMBEDDING_PER_HEAD])
[MultiHeadAttention:3] Energy = Q * K (raw attention scores) shape: torch.Size([BATCH_SIZE, HEAD_COUNT, SEQ_LEN, SEQ_LEN])
[MultiHeadAttention:4] Applying mask with shape: torch.Size([BATCH_SIZE, 1, 1, SEQ_LEN])
[MultiHeadAttention:5] Attention SoftMax weights shape: torch.Size([BATCH_SIZE, HEAD_COUNT, SEQ_LEN, SEQ_LEN])
[MultiHeadAttention:6] Attention * V = Output shape after weighted sum: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])
[TransformerBlock] After attention and residual connection: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])
[FeedForward] Input shape: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM]), Performing feedforward transformations.
[FeedForward] Output shape: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])
[TransformerBlock] After feedforward and residual connection: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])

[TransformerEncoder] Processing layer 6
[TransformerBlock] Starting block processing. Input dimensions - value: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM]), key: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM]), query: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])
[MultiHeadAttention:1] Input shapes (V, K, Q) - values: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM]), keys: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM]), queries: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])
[MultiHeadAttention:2] After linear projections V/K/Q multihead- values: torch.Size([BATCH_SIZE, SEQ_LEN, HEAD_COUNT, EMBEDDING_PER_HEAD]), keys: torch.Size([BATCH_SIZE, SEQ_LEN, HEAD_COUNT, EMBEDDING_PER_HEAD]), queries: torch.Size([BATCH_SIZE, SEQ_LEN, HEAD_COUNT, EMBEDDING_PER_HEAD])
[MultiHeadAttention:3] Energy = Q * K (raw attention scores) shape: torch.Size([BATCH_SIZE, HEAD_COUNT, SEQ_LEN, SEQ_LEN])
[MultiHeadAttention:4] Applying mask with shape: torch.Size([BATCH_SIZE, 1, 1, SEQ_LEN])
[MultiHeadAttention:5] Attention SoftMax weights shape: torch.Size([BATCH_SIZE, HEAD_COUNT, SEQ_LEN, SEQ_LEN])
[MultiHeadAttention:6] Attention * V = Output shape after weighted sum: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])
[TransformerBlock] After attention and residual connection: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])
[FeedForward] Input shape: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM]), Performing feedforward transformations.
[FeedForward] Output shape: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])
[TransformerBlock] After feedforward and residual connection: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])

[TransformerEncoder] Processing layer 7
[TransformerBlock] Starting block processing. Input dimensions - value: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM]), key: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM]), query: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])
[MultiHeadAttention:1] Input shapes (V, K, Q) - values: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM]), keys: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM]), queries: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])
[MultiHeadAttention:2] After linear projections V/K/Q multihead- values: torch.Size([BATCH_SIZE, SEQ_LEN, HEAD_COUNT, EMBEDDING_PER_HEAD]), keys: torch.Size([BATCH_SIZE, SEQ_LEN, HEAD_COUNT, EMBEDDING_PER_HEAD]), queries: torch.Size([BATCH_SIZE, SEQ_LEN, HEAD_COUNT, EMBEDDING_PER_HEAD])
[MultiHeadAttention:3] Energy = Q * K (raw attention scores) shape: torch.Size([BATCH_SIZE, HEAD_COUNT, SEQ_LEN, SEQ_LEN])
[MultiHeadAttention:4] Applying mask with shape: torch.Size([BATCH_SIZE, 1, 1, SEQ_LEN])
[MultiHeadAttention:5] Attention SoftMax weights shape: torch.Size([BATCH_SIZE, HEAD_COUNT, SEQ_LEN, SEQ_LEN])
[MultiHeadAttention:6] Attention * V = Output shape after weighted sum: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])
[TransformerBlock] After attention and residual connection: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])
[FeedForward] Input shape: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM]), Performing feedforward transformations.
[FeedForward] Output shape: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])
[TransformerBlock] After feedforward and residual connection: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])

[TransformerEncoder] Processing layer 8
[TransformerBlock] Starting block processing. Input dimensions - value: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM]), key: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM]), query: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])
[MultiHeadAttention:1] Input shapes (V, K, Q) - values: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM]), keys: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM]), queries: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])
[MultiHeadAttention:2] After linear projections V/K/Q multihead- values: torch.Size([BATCH_SIZE, SEQ_LEN, HEAD_COUNT, EMBEDDING_PER_HEAD]), keys: torch.Size([BATCH_SIZE, SEQ_LEN, HEAD_COUNT, EMBEDDING_PER_HEAD]), queries: torch.Size([BATCH_SIZE, SEQ_LEN, HEAD_COUNT, EMBEDDING_PER_HEAD])
[MultiHeadAttention:3] Energy = Q * K (raw attention scores) shape: torch.Size([BATCH_SIZE, HEAD_COUNT, SEQ_LEN, SEQ_LEN])
[MultiHeadAttention:4] Applying mask with shape: torch.Size([BATCH_SIZE, 1, 1, SEQ_LEN])
[MultiHeadAttention:5] Attention SoftMax weights shape: torch.Size([BATCH_SIZE, HEAD_COUNT, SEQ_LEN, SEQ_LEN])
[MultiHeadAttention:6] Attention * V = Output shape after weighted sum: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])
[TransformerBlock] After attention and residual connection: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])
[FeedForward] Input shape: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM]), Performing feedforward transformations.
[FeedForward] Output shape: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])
[TransformerBlock] After feedforward and residual connection: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])

[TransformerEncoder] Processing layer 9
[TransformerBlock] Starting block processing. Input dimensions - value: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM]), key: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM]), query: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])
[MultiHeadAttention:1] Input shapes (V, K, Q) - values: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM]), keys: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM]), queries: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])
[MultiHeadAttention:2] After linear projections V/K/Q multihead- values: torch.Size([BATCH_SIZE, SEQ_LEN, HEAD_COUNT, EMBEDDING_PER_HEAD]), keys: torch.Size([BATCH_SIZE, SEQ_LEN, HEAD_COUNT, EMBEDDING_PER_HEAD]), queries: torch.Size([BATCH_SIZE, SEQ_LEN, HEAD_COUNT, EMBEDDING_PER_HEAD])
[MultiHeadAttention:3] Energy = Q * K (raw attention scores) shape: torch.Size([BATCH_SIZE, HEAD_COUNT, SEQ_LEN, SEQ_LEN])
[MultiHeadAttention:4] Applying mask with shape: torch.Size([BATCH_SIZE, 1, 1, SEQ_LEN])
[MultiHeadAttention:5] Attention SoftMax weights shape: torch.Size([BATCH_SIZE, HEAD_COUNT, SEQ_LEN, SEQ_LEN])
[MultiHeadAttention:6] Attention * V = Output shape after weighted sum: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])
[TransformerBlock] After attention and residual connection: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])
[FeedForward] Input shape: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM]), Performing feedforward transformations.
[FeedForward] Output shape: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])
[TransformerBlock] After feedforward and residual connection: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])

[TransformerEncoder] Processing layer 10
[TransformerBlock] Starting block processing. Input dimensions - value: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM]), key: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM]), query: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])
[MultiHeadAttention:1] Input shapes (V, K, Q) - values: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM]), keys: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM]), queries: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])
[MultiHeadAttention:2] After linear projections V/K/Q multihead- values: torch.Size([BATCH_SIZE, SEQ_LEN, HEAD_COUNT, EMBEDDING_PER_HEAD]), keys: torch.Size([BATCH_SIZE, SEQ_LEN, HEAD_COUNT, EMBEDDING_PER_HEAD]), queries: torch.Size([BATCH_SIZE, SEQ_LEN, HEAD_COUNT, EMBEDDING_PER_HEAD])
[MultiHeadAttention:3] Energy = Q * K (raw attention scores) shape: torch.Size([BATCH_SIZE, HEAD_COUNT, SEQ_LEN, SEQ_LEN])
[MultiHeadAttention:4] Applying mask with shape: torch.Size([BATCH_SIZE, 1, 1, SEQ_LEN])
[MultiHeadAttention:5] Attention SoftMax weights shape: torch.Size([BATCH_SIZE, HEAD_COUNT, SEQ_LEN, SEQ_LEN])
[MultiHeadAttention:6] Attention * V = Output shape after weighted sum: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])
[TransformerBlock] After attention and residual connection: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])
[FeedForward] Input shape: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM]), Performing feedforward transformations.
[FeedForward] Output shape: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])
[TransformerBlock] After feedforward and residual connection: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])

[TransformerEncoder] Processing layer SEQ_LEN
[TransformerBlock] Starting block processing. Input dimensions - value: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM]), key: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM]), query: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])
[MultiHeadAttention:1] Input shapes (V, K, Q) - values: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM]), keys: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM]), queries: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])
[MultiHeadAttention:2] After linear projections V/K/Q multihead- values: torch.Size([BATCH_SIZE, SEQ_LEN, HEAD_COUNT, EMBEDDING_PER_HEAD]), keys: torch.Size([BATCH_SIZE, SEQ_LEN, HEAD_COUNT, EMBEDDING_PER_HEAD]), queries: torch.Size([BATCH_SIZE, SEQ_LEN, HEAD_COUNT, EMBEDDING_PER_HEAD])
[MultiHeadAttention:3] Energy = Q * K (raw attention scores) shape: torch.Size([BATCH_SIZE, HEAD_COUNT, SEQ_LEN, SEQ_LEN])
[MultiHeadAttention:4] Applying mask with shape: torch.Size([BATCH_SIZE, 1, 1, SEQ_LEN])
[MultiHeadAttention:5] Attention SoftMax weights shape: torch.Size([BATCH_SIZE, HEAD_COUNT, SEQ_LEN, SEQ_LEN])
[MultiHeadAttention:6] Attention * V = Output shape after weighted sum: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])
[TransformerBlock] After attention and residual connection: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])
[FeedForward] Input shape: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM]), Performing feedforward transformations.
[FeedForward] Output shape: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])
[TransformerBlock] After feedforward and residual connection: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])

[TransformerEncoder] Processing layer EMBEDDING_PER_HEAD
[TransformerBlock] Starting block processing. Input dimensions - value: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM]), key: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM]), query: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])
[MultiHeadAttention:1] Input shapes (V, K, Q) - values: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM]), keys: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM]), queries: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])
[MultiHeadAttention:2] After linear projections V/K/Q multihead- values: torch.Size([BATCH_SIZE, SEQ_LEN, HEAD_COUNT, EMBEDDING_PER_HEAD]), keys: torch.Size([BATCH_SIZE, SEQ_LEN, HEAD_COUNT, EMBEDDING_PER_HEAD]), queries: torch.Size([BATCH_SIZE, SEQ_LEN, HEAD_COUNT, EMBEDDING_PER_HEAD])
[MultiHeadAttention:3] Energy = Q * K (raw attention scores) shape: torch.Size([BATCH_SIZE, HEAD_COUNT, SEQ_LEN, SEQ_LEN])
[MultiHeadAttention:4] Applying mask with shape: torch.Size([BATCH_SIZE, 1, 1, SEQ_LEN])
[MultiHeadAttention:5] Attention SoftMax weights shape: torch.Size([BATCH_SIZE, HEAD_COUNT, SEQ_LEN, SEQ_LEN])
[MultiHeadAttention:6] Attention * V = Output shape after weighted sum: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])
[TransformerBlock] After attention and residual connection: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])
[FeedForward] Input shape: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM]), Performing feedforward transformations.
[FeedForward] Output shape: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])
[TransformerBlock] After feedforward and residual connection: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])
[TransformerEncoder] Output x.shape: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])
[TransformerDecoder] Input x.shape: torch.Size([BATCH_SIZE, SEQ_LEN]), encoder_out.shape: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])
[PositionalEncoding] Input x.shape: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM]), Adding positional encodings for sequence length and embedding size. peshape torch.Size([1, MAX_LEN, EMBEDDING_DIM])
while adding, pe[:, :x.size(1)]: torch.Size([1, SEQ_LEN, EMBEDDING_DIM])
[PositionalEncoding] Output shape: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])

[TransformerDecoder] Processing layer 1
[TransformerDecoderBlock] x.shape: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM]), encoder_out.shape: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])
[MultiHeadAttention:1] Input shapes (V, K, Q) - values: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM]), keys: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM]), queries: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])
[MultiHeadAttention:2] After linear projections V/K/Q multihead- values: torch.Size([BATCH_SIZE, SEQ_LEN, HEAD_COUNT, EMBEDDING_PER_HEAD]), keys: torch.Size([BATCH_SIZE, SEQ_LEN, HEAD_COUNT, EMBEDDING_PER_HEAD]), queries: torch.Size([BATCH_SIZE, SEQ_LEN, HEAD_COUNT, EMBEDDING_PER_HEAD])
[MultiHeadAttention:3] Energy = Q * K (raw attention scores) shape: torch.Size([BATCH_SIZE, HEAD_COUNT, SEQ_LEN, SEQ_LEN])
[MultiHeadAttention:4] Applying mask with shape: torch.Size([BATCH_SIZE, 1, SEQ_LEN, SEQ_LEN])
[MultiHeadAttention:5] Attention SoftMax weights shape: torch.Size([BATCH_SIZE, HEAD_COUNT, SEQ_LEN, SEQ_LEN])
[MultiHeadAttention:6] Attention * V = Output shape after weighted sum: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])
[MultiHeadAttention:1] Input shapes (V, K, Q) - values: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM]), keys: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM]), queries: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])
[MultiHeadAttention:2] After linear projections V/K/Q multihead- values: torch.Size([BATCH_SIZE, SEQ_LEN, HEAD_COUNT, EMBEDDING_PER_HEAD]), keys: torch.Size([BATCH_SIZE, SEQ_LEN, HEAD_COUNT, EMBEDDING_PER_HEAD]), queries: torch.Size([BATCH_SIZE, SEQ_LEN, HEAD_COUNT, EMBEDDING_PER_HEAD])
[MultiHeadAttention:3] Energy = Q * K (raw attention scores) shape: torch.Size([BATCH_SIZE, HEAD_COUNT, SEQ_LEN, SEQ_LEN])
[MultiHeadAttention:4] Applying mask with shape: torch.Size([BATCH_SIZE, 1, 1, SEQ_LEN])
[MultiHeadAttention:5] Attention SoftMax weights shape: torch.Size([BATCH_SIZE, HEAD_COUNT, SEQ_LEN, SEQ_LEN])
[MultiHeadAttention:6] Attention * V = Output shape after weighted sum: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])
[FeedForward] Input shape: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM]), Performing feedforward transformations.
[FeedForward] Output shape: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])
[TransformerDecoderBlock] out.shape: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])

[TransformerDecoder] Processing layer 2
[TransformerDecoderBlock] x.shape: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM]), encoder_out.shape: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])
[MultiHeadAttention:1] Input shapes (V, K, Q) - values: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM]), keys: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM]), queries: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])
[MultiHeadAttention:2] After linear projections V/K/Q multihead- values: torch.Size([BATCH_SIZE, SEQ_LEN, HEAD_COUNT, EMBEDDING_PER_HEAD]), keys: torch.Size([BATCH_SIZE, SEQ_LEN, HEAD_COUNT, EMBEDDING_PER_HEAD]), queries: torch.Size([BATCH_SIZE, SEQ_LEN, HEAD_COUNT, EMBEDDING_PER_HEAD])
[MultiHeadAttention:3] Energy = Q * K (raw attention scores) shape: torch.Size([BATCH_SIZE, HEAD_COUNT, SEQ_LEN, SEQ_LEN])
[MultiHeadAttention:4] Applying mask with shape: torch.Size([BATCH_SIZE, 1, SEQ_LEN, SEQ_LEN])
[MultiHeadAttention:5] Attention SoftMax weights shape: torch.Size([BATCH_SIZE, HEAD_COUNT, SEQ_LEN, SEQ_LEN])
[MultiHeadAttention:6] Attention * V = Output shape after weighted sum: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])
[MultiHeadAttention:1] Input shapes (V, K, Q) - values: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM]), keys: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM]), queries: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])
[MultiHeadAttention:2] After linear projections V/K/Q multihead- values: torch.Size([BATCH_SIZE, SEQ_LEN, HEAD_COUNT, EMBEDDING_PER_HEAD]), keys: torch.Size([BATCH_SIZE, SEQ_LEN, HEAD_COUNT, EMBEDDING_PER_HEAD]), queries: torch.Size([BATCH_SIZE, SEQ_LEN, HEAD_COUNT, EMBEDDING_PER_HEAD])
[MultiHeadAttention:3] Energy = Q * K (raw attention scores) shape: torch.Size([BATCH_SIZE, HEAD_COUNT, SEQ_LEN, SEQ_LEN])
[MultiHeadAttention:4] Applying mask with shape: torch.Size([BATCH_SIZE, 1, 1, SEQ_LEN])
[MultiHeadAttention:5] Attention SoftMax weights shape: torch.Size([BATCH_SIZE, HEAD_COUNT, SEQ_LEN, SEQ_LEN])
[MultiHeadAttention:6] Attention * V = Output shape after weighted sum: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])
[FeedForward] Input shape: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM]), Performing feedforward transformations.
[FeedForward] Output shape: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])
[TransformerDecoderBlock] out.shape: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])

[TransformerDecoder] Processing layer 3
[TransformerDecoderBlock] x.shape: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM]), encoder_out.shape: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])
[MultiHeadAttention:1] Input shapes (V, K, Q) - values: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM]), keys: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM]), queries: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])
[MultiHeadAttention:2] After linear projections V/K/Q multihead- values: torch.Size([BATCH_SIZE, SEQ_LEN, HEAD_COUNT, EMBEDDING_PER_HEAD]), keys: torch.Size([BATCH_SIZE, SEQ_LEN, HEAD_COUNT, EMBEDDING_PER_HEAD]), queries: torch.Size([BATCH_SIZE, SEQ_LEN, HEAD_COUNT, EMBEDDING_PER_HEAD])
[MultiHeadAttention:3] Energy = Q * K (raw attention scores) shape: torch.Size([BATCH_SIZE, HEAD_COUNT, SEQ_LEN, SEQ_LEN])
[MultiHeadAttention:4] Applying mask with shape: torch.Size([BATCH_SIZE, 1, SEQ_LEN, SEQ_LEN])
[MultiHeadAttention:5] Attention SoftMax weights shape: torch.Size([BATCH_SIZE, HEAD_COUNT, SEQ_LEN, SEQ_LEN])
[MultiHeadAttention:6] Attention * V = Output shape after weighted sum: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])
[MultiHeadAttention:1] Input shapes (V, K, Q) - values: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM]), keys: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM]), queries: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])
[MultiHeadAttention:2] After linear projections V/K/Q multihead- values: torch.Size([BATCH_SIZE, SEQ_LEN, HEAD_COUNT, EMBEDDING_PER_HEAD]), keys: torch.Size([BATCH_SIZE, SEQ_LEN, HEAD_COUNT, EMBEDDING_PER_HEAD]), queries: torch.Size([BATCH_SIZE, SEQ_LEN, HEAD_COUNT, EMBEDDING_PER_HEAD])
[MultiHeadAttention:3] Energy = Q * K (raw attention scores) shape: torch.Size([BATCH_SIZE, HEAD_COUNT, SEQ_LEN, SEQ_LEN])
[MultiHeadAttention:4] Applying mask with shape: torch.Size([BATCH_SIZE, 1, 1, SEQ_LEN])
[MultiHeadAttention:5] Attention SoftMax weights shape: torch.Size([BATCH_SIZE, HEAD_COUNT, SEQ_LEN, SEQ_LEN])
[MultiHeadAttention:6] Attention * V = Output shape after weighted sum: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])
[FeedForward] Input shape: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM]), Performing feedforward transformations.
[FeedForward] Output shape: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])
[TransformerDecoderBlock] out.shape: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])

[TransformerDecoder] Processing layer 4
[TransformerDecoderBlock] x.shape: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM]), encoder_out.shape: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])
[MultiHeadAttention:1] Input shapes (V, K, Q) - values: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM]), keys: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM]), queries: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])
[MultiHeadAttention:2] After linear projections V/K/Q multihead- values: torch.Size([BATCH_SIZE, SEQ_LEN, HEAD_COUNT, EMBEDDING_PER_HEAD]), keys: torch.Size([BATCH_SIZE, SEQ_LEN, HEAD_COUNT, EMBEDDING_PER_HEAD]), queries: torch.Size([BATCH_SIZE, SEQ_LEN, HEAD_COUNT, EMBEDDING_PER_HEAD])
[MultiHeadAttention:3] Energy = Q * K (raw attention scores) shape: torch.Size([BATCH_SIZE, HEAD_COUNT, SEQ_LEN, SEQ_LEN])
[MultiHeadAttention:4] Applying mask with shape: torch.Size([BATCH_SIZE, 1, SEQ_LEN, SEQ_LEN])
[MultiHeadAttention:5] Attention SoftMax weights shape: torch.Size([BATCH_SIZE, HEAD_COUNT, SEQ_LEN, SEQ_LEN])
[MultiHeadAttention:6] Attention * V = Output shape after weighted sum: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])
[MultiHeadAttention:1] Input shapes (V, K, Q) - values: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM]), keys: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM]), queries: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])
[MultiHeadAttention:2] After linear projections V/K/Q multihead- values: torch.Size([BATCH_SIZE, SEQ_LEN, HEAD_COUNT, EMBEDDING_PER_HEAD]), keys: torch.Size([BATCH_SIZE, SEQ_LEN, HEAD_COUNT, EMBEDDING_PER_HEAD]), queries: torch.Size([BATCH_SIZE, SEQ_LEN, HEAD_COUNT, EMBEDDING_PER_HEAD])
[MultiHeadAttention:3] Energy = Q * K (raw attention scores) shape: torch.Size([BATCH_SIZE, HEAD_COUNT, SEQ_LEN, SEQ_LEN])
[MultiHeadAttention:4] Applying mask with shape: torch.Size([BATCH_SIZE, 1, 1, SEQ_LEN])
[MultiHeadAttention:5] Attention SoftMax weights shape: torch.Size([BATCH_SIZE, HEAD_COUNT, SEQ_LEN, SEQ_LEN])
[MultiHeadAttention:6] Attention * V = Output shape after weighted sum: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])
[FeedForward] Input shape: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM]), Performing feedforward transformations.
[FeedForward] Output shape: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])
[TransformerDecoderBlock] out.shape: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])

[TransformerDecoder] Processing layer 5
[TransformerDecoderBlock] x.shape: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM]), encoder_out.shape: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])
[MultiHeadAttention:1] Input shapes (V, K, Q) - values: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM]), keys: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM]), queries: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])
[MultiHeadAttention:2] After linear projections V/K/Q multihead- values: torch.Size([BATCH_SIZE, SEQ_LEN, HEAD_COUNT, EMBEDDING_PER_HEAD]), keys: torch.Size([BATCH_SIZE, SEQ_LEN, HEAD_COUNT, EMBEDDING_PER_HEAD]), queries: torch.Size([BATCH_SIZE, SEQ_LEN, HEAD_COUNT, EMBEDDING_PER_HEAD])
[MultiHeadAttention:3] Energy = Q * K (raw attention scores) shape: torch.Size([BATCH_SIZE, HEAD_COUNT, SEQ_LEN, SEQ_LEN])
[MultiHeadAttention:4] Applying mask with shape: torch.Size([BATCH_SIZE, 1, SEQ_LEN, SEQ_LEN])
[MultiHeadAttention:5] Attention SoftMax weights shape: torch.Size([BATCH_SIZE, HEAD_COUNT, SEQ_LEN, SEQ_LEN])
[MultiHeadAttention:6] Attention * V = Output shape after weighted sum: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])
[MultiHeadAttention:1] Input shapes (V, K, Q) - values: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM]), keys: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM]), queries: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])
[MultiHeadAttention:2] After linear projections V/K/Q multihead- values: torch.Size([BATCH_SIZE, SEQ_LEN, HEAD_COUNT, EMBEDDING_PER_HEAD]), keys: torch.Size([BATCH_SIZE, SEQ_LEN, HEAD_COUNT, EMBEDDING_PER_HEAD]), queries: torch.Size([BATCH_SIZE, SEQ_LEN, HEAD_COUNT, EMBEDDING_PER_HEAD])
[MultiHeadAttention:3] Energy = Q * K (raw attention scores) shape: torch.Size([BATCH_SIZE, HEAD_COUNT, SEQ_LEN, SEQ_LEN])
[MultiHeadAttention:4] Applying mask with shape: torch.Size([BATCH_SIZE, 1, 1, SEQ_LEN])
[MultiHeadAttention:5] Attention SoftMax weights shape: torch.Size([BATCH_SIZE, HEAD_COUNT, SEQ_LEN, SEQ_LEN])
[MultiHeadAttention:6] Attention * V = Output shape after weighted sum: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])
[FeedForward] Input shape: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM]), Performing feedforward transformations.
[FeedForward] Output shape: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])
[TransformerDecoderBlock] out.shape: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])

[TransformerDecoder] Processing layer 6
[TransformerDecoderBlock] x.shape: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM]), encoder_out.shape: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])
[MultiHeadAttention:1] Input shapes (V, K, Q) - values: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM]), keys: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM]), queries: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])
[MultiHeadAttention:2] After linear projections V/K/Q multihead- values: torch.Size([BATCH_SIZE, SEQ_LEN, HEAD_COUNT, EMBEDDING_PER_HEAD]), keys: torch.Size([BATCH_SIZE, SEQ_LEN, HEAD_COUNT, EMBEDDING_PER_HEAD]), queries: torch.Size([BATCH_SIZE, SEQ_LEN, HEAD_COUNT, EMBEDDING_PER_HEAD])
[MultiHeadAttention:3] Energy = Q * K (raw attention scores) shape: torch.Size([BATCH_SIZE, HEAD_COUNT, SEQ_LEN, SEQ_LEN])
[MultiHeadAttention:4] Applying mask with shape: torch.Size([BATCH_SIZE, 1, SEQ_LEN, SEQ_LEN])
[MultiHeadAttention:5] Attention SoftMax weights shape: torch.Size([BATCH_SIZE, HEAD_COUNT, SEQ_LEN, SEQ_LEN])
[MultiHeadAttention:6] Attention * V = Output shape after weighted sum: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])
[MultiHeadAttention:1] Input shapes (V, K, Q) - values: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM]), keys: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM]), queries: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])
[MultiHeadAttention:2] After linear projections V/K/Q multihead- values: torch.Size([BATCH_SIZE, SEQ_LEN, HEAD_COUNT, EMBEDDING_PER_HEAD]), keys: torch.Size([BATCH_SIZE, SEQ_LEN, HEAD_COUNT, EMBEDDING_PER_HEAD]), queries: torch.Size([BATCH_SIZE, SEQ_LEN, HEAD_COUNT, EMBEDDING_PER_HEAD])
[MultiHeadAttention:3] Energy = Q * K (raw attention scores) shape: torch.Size([BATCH_SIZE, HEAD_COUNT, SEQ_LEN, SEQ_LEN])
[MultiHeadAttention:4] Applying mask with shape: torch.Size([BATCH_SIZE, 1, 1, SEQ_LEN])
[MultiHeadAttention:5] Attention SoftMax weights shape: torch.Size([BATCH_SIZE, HEAD_COUNT, SEQ_LEN, SEQ_LEN])
[MultiHeadAttention:6] Attention * V = Output shape after weighted sum: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])
[FeedForward] Input shape: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM]), Performing feedforward transformations.
[FeedForward] Output shape: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])
[TransformerDecoderBlock] out.shape: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])

[TransformerDecoder] Processing layer 7
[TransformerDecoderBlock] x.shape: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM]), encoder_out.shape: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])
[MultiHeadAttention:1] Input shapes (V, K, Q) - values: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM]), keys: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM]), queries: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])
[MultiHeadAttention:2] After linear projections V/K/Q multihead- values: torch.Size([BATCH_SIZE, SEQ_LEN, HEAD_COUNT, EMBEDDING_PER_HEAD]), keys: torch.Size([BATCH_SIZE, SEQ_LEN, HEAD_COUNT, EMBEDDING_PER_HEAD]), queries: torch.Size([BATCH_SIZE, SEQ_LEN, HEAD_COUNT, EMBEDDING_PER_HEAD])
[MultiHeadAttention:3] Energy = Q * K (raw attention scores) shape: torch.Size([BATCH_SIZE, HEAD_COUNT, SEQ_LEN, SEQ_LEN])
[MultiHeadAttention:4] Applying mask with shape: torch.Size([BATCH_SIZE, 1, SEQ_LEN, SEQ_LEN])
[MultiHeadAttention:5] Attention SoftMax weights shape: torch.Size([BATCH_SIZE, HEAD_COUNT, SEQ_LEN, SEQ_LEN])
[MultiHeadAttention:6] Attention * V = Output shape after weighted sum: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])
[MultiHeadAttention:1] Input shapes (V, K, Q) - values: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM]), keys: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM]), queries: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])
[MultiHeadAttention:2] After linear projections V/K/Q multihead- values: torch.Size([BATCH_SIZE, SEQ_LEN, HEAD_COUNT, EMBEDDING_PER_HEAD]), keys: torch.Size([BATCH_SIZE, SEQ_LEN, HEAD_COUNT, EMBEDDING_PER_HEAD]), queries: torch.Size([BATCH_SIZE, SEQ_LEN, HEAD_COUNT, EMBEDDING_PER_HEAD])
[MultiHeadAttention:3] Energy = Q * K (raw attention scores) shape: torch.Size([BATCH_SIZE, HEAD_COUNT, SEQ_LEN, SEQ_LEN])
[MultiHeadAttention:4] Applying mask with shape: torch.Size([BATCH_SIZE, 1, 1, SEQ_LEN])
[MultiHeadAttention:5] Attention SoftMax weights shape: torch.Size([BATCH_SIZE, HEAD_COUNT, SEQ_LEN, SEQ_LEN])
[MultiHeadAttention:6] Attention * V = Output shape after weighted sum: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])
[FeedForward] Input shape: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM]), Performing feedforward transformations.
[FeedForward] Output shape: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])
[TransformerDecoderBlock] out.shape: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])

[TransformerDecoder] Processing layer 8
[TransformerDecoderBlock] x.shape: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM]), encoder_out.shape: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])
[MultiHeadAttention:1] Input shapes (V, K, Q) - values: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM]), keys: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM]), queries: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])
[MultiHeadAttention:2] After linear projections V/K/Q multihead- values: torch.Size([BATCH_SIZE, SEQ_LEN, HEAD_COUNT, EMBEDDING_PER_HEAD]), keys: torch.Size([BATCH_SIZE, SEQ_LEN, HEAD_COUNT, EMBEDDING_PER_HEAD]), queries: torch.Size([BATCH_SIZE, SEQ_LEN, HEAD_COUNT, EMBEDDING_PER_HEAD])
[MultiHeadAttention:3] Energy = Q * K (raw attention scores) shape: torch.Size([BATCH_SIZE, HEAD_COUNT, SEQ_LEN, SEQ_LEN])
[MultiHeadAttention:4] Applying mask with shape: torch.Size([BATCH_SIZE, 1, SEQ_LEN, SEQ_LEN])
[MultiHeadAttention:5] Attention SoftMax weights shape: torch.Size([BATCH_SIZE, HEAD_COUNT, SEQ_LEN, SEQ_LEN])
[MultiHeadAttention:6] Attention * V = Output shape after weighted sum: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])
[MultiHeadAttention:1] Input shapes (V, K, Q) - values: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM]), keys: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM]), queries: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])
[MultiHeadAttention:2] After linear projections V/K/Q multihead- values: torch.Size([BATCH_SIZE, SEQ_LEN, HEAD_COUNT, EMBEDDING_PER_HEAD]), keys: torch.Size([BATCH_SIZE, SEQ_LEN, HEAD_COUNT, EMBEDDING_PER_HEAD]), queries: torch.Size([BATCH_SIZE, SEQ_LEN, HEAD_COUNT, EMBEDDING_PER_HEAD])
[MultiHeadAttention:3] Energy = Q * K (raw attention scores) shape: torch.Size([BATCH_SIZE, HEAD_COUNT, SEQ_LEN, SEQ_LEN])
[MultiHeadAttention:4] Applying mask with shape: torch.Size([BATCH_SIZE, 1, 1, SEQ_LEN])
[MultiHeadAttention:5] Attention SoftMax weights shape: torch.Size([BATCH_SIZE, HEAD_COUNT, SEQ_LEN, SEQ_LEN])
[MultiHeadAttention:6] Attention * V = Output shape after weighted sum: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])
[FeedForward] Input shape: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM]), Performing feedforward transformations.
[FeedForward] Output shape: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])
[TransformerDecoderBlock] out.shape: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])

[TransformerDecoder] Processing layer 9
[TransformerDecoderBlock] x.shape: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM]), encoder_out.shape: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])
[MultiHeadAttention:1] Input shapes (V, K, Q) - values: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM]), keys: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM]), queries: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])
[MultiHeadAttention:2] After linear projections V/K/Q multihead- values: torch.Size([BATCH_SIZE, SEQ_LEN, HEAD_COUNT, EMBEDDING_PER_HEAD]), keys: torch.Size([BATCH_SIZE, SEQ_LEN, HEAD_COUNT, EMBEDDING_PER_HEAD]), queries: torch.Size([BATCH_SIZE, SEQ_LEN, HEAD_COUNT, EMBEDDING_PER_HEAD])
[MultiHeadAttention:3] Energy = Q * K (raw attention scores) shape: torch.Size([BATCH_SIZE, HEAD_COUNT, SEQ_LEN, SEQ_LEN])
[MultiHeadAttention:4] Applying mask with shape: torch.Size([BATCH_SIZE, 1, SEQ_LEN, SEQ_LEN])
[MultiHeadAttention:5] Attention SoftMax weights shape: torch.Size([BATCH_SIZE, HEAD_COUNT, SEQ_LEN, SEQ_LEN])
[MultiHeadAttention:6] Attention * V = Output shape after weighted sum: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])
[MultiHeadAttention:1] Input shapes (V, K, Q) - values: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM]), keys: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM]), queries: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])
[MultiHeadAttention:2] After linear projections V/K/Q multihead- values: torch.Size([BATCH_SIZE, SEQ_LEN, HEAD_COUNT, EMBEDDING_PER_HEAD]), keys: torch.Size([BATCH_SIZE, SEQ_LEN, HEAD_COUNT, EMBEDDING_PER_HEAD]), queries: torch.Size([BATCH_SIZE, SEQ_LEN, HEAD_COUNT, EMBEDDING_PER_HEAD])
[MultiHeadAttention:3] Energy = Q * K (raw attention scores) shape: torch.Size([BATCH_SIZE, HEAD_COUNT, SEQ_LEN, SEQ_LEN])
[MultiHeadAttention:4] Applying mask with shape: torch.Size([BATCH_SIZE, 1, 1, SEQ_LEN])
[MultiHeadAttention:5] Attention SoftMax weights shape: torch.Size([BATCH_SIZE, HEAD_COUNT, SEQ_LEN, SEQ_LEN])
[MultiHeadAttention:6] Attention * V = Output shape after weighted sum: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])
[FeedForward] Input shape: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM]), Performing feedforward transformations.
[FeedForward] Output shape: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])
[TransformerDecoderBlock] out.shape: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])

[TransformerDecoder] Processing layer 10
[TransformerDecoderBlock] x.shape: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM]), encoder_out.shape: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])
[MultiHeadAttention:1] Input shapes (V, K, Q) - values: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM]), keys: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM]), queries: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])
[MultiHeadAttention:2] After linear projections V/K/Q multihead- values: torch.Size([BATCH_SIZE, SEQ_LEN, HEAD_COUNT, EMBEDDING_PER_HEAD]), keys: torch.Size([BATCH_SIZE, SEQ_LEN, HEAD_COUNT, EMBEDDING_PER_HEAD]), queries: torch.Size([BATCH_SIZE, SEQ_LEN, HEAD_COUNT, EMBEDDING_PER_HEAD])
[MultiHeadAttention:3] Energy = Q * K (raw attention scores) shape: torch.Size([BATCH_SIZE, HEAD_COUNT, SEQ_LEN, SEQ_LEN])
[MultiHeadAttention:4] Applying mask with shape: torch.Size([BATCH_SIZE, 1, SEQ_LEN, SEQ_LEN])
[MultiHeadAttention:5] Attention SoftMax weights shape: torch.Size([BATCH_SIZE, HEAD_COUNT, SEQ_LEN, SEQ_LEN])
[MultiHeadAttention:6] Attention * V = Output shape after weighted sum: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])
[MultiHeadAttention:1] Input shapes (V, K, Q) - values: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM]), keys: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM]), queries: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])
[MultiHeadAttention:2] After linear projections V/K/Q multihead- values: torch.Size([BATCH_SIZE, SEQ_LEN, HEAD_COUNT, EMBEDDING_PER_HEAD]), keys: torch.Size([BATCH_SIZE, SEQ_LEN, HEAD_COUNT, EMBEDDING_PER_HEAD]), queries: torch.Size([BATCH_SIZE, SEQ_LEN, HEAD_COUNT, EMBEDDING_PER_HEAD])
[MultiHeadAttention:3] Energy = Q * K (raw attention scores) shape: torch.Size([BATCH_SIZE, HEAD_COUNT, SEQ_LEN, SEQ_LEN])
[MultiHeadAttention:4] Applying mask with shape: torch.Size([BATCH_SIZE, 1, 1, SEQ_LEN])
[MultiHeadAttention:5] Attention SoftMax weights shape: torch.Size([BATCH_SIZE, HEAD_COUNT, SEQ_LEN, SEQ_LEN])
[MultiHeadAttention:6] Attention * V = Output shape after weighted sum: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])
[FeedForward] Input shape: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM]), Performing feedforward transformations.
[FeedForward] Output shape: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])
[TransformerDecoderBlock] out.shape: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])

[TransformerDecoder] Processing layer SEQ_LEN
[TransformerDecoderBlock] x.shape: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM]), encoder_out.shape: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])
[MultiHeadAttention:1] Input shapes (V, K, Q) - values: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM]), keys: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM]), queries: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])
[MultiHeadAttention:2] After linear projections V/K/Q multihead- values: torch.Size([BATCH_SIZE, SEQ_LEN, HEAD_COUNT, EMBEDDING_PER_HEAD]), keys: torch.Size([BATCH_SIZE, SEQ_LEN, HEAD_COUNT, EMBEDDING_PER_HEAD]), queries: torch.Size([BATCH_SIZE, SEQ_LEN, HEAD_COUNT, EMBEDDING_PER_HEAD])
[MultiHeadAttention:3] Energy = Q * K (raw attention scores) shape: torch.Size([BATCH_SIZE, HEAD_COUNT, SEQ_LEN, SEQ_LEN])
[MultiHeadAttention:4] Applying mask with shape: torch.Size([BATCH_SIZE, 1, SEQ_LEN, SEQ_LEN])
[MultiHeadAttention:5] Attention SoftMax weights shape: torch.Size([BATCH_SIZE, HEAD_COUNT, SEQ_LEN, SEQ_LEN])
[MultiHeadAttention:6] Attention * V = Output shape after weighted sum: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])
[MultiHeadAttention:1] Input shapes (V, K, Q) - values: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM]), keys: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM]), queries: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])
[MultiHeadAttention:2] After linear projections V/K/Q multihead- values: torch.Size([BATCH_SIZE, SEQ_LEN, HEAD_COUNT, EMBEDDING_PER_HEAD]), keys: torch.Size([BATCH_SIZE, SEQ_LEN, HEAD_COUNT, EMBEDDING_PER_HEAD]), queries: torch.Size([BATCH_SIZE, SEQ_LEN, HEAD_COUNT, EMBEDDING_PER_HEAD])
[MultiHeadAttention:3] Energy = Q * K (raw attention scores) shape: torch.Size([BATCH_SIZE, HEAD_COUNT, SEQ_LEN, SEQ_LEN])
[MultiHeadAttention:4] Applying mask with shape: torch.Size([BATCH_SIZE, 1, 1, SEQ_LEN])
[MultiHeadAttention:5] Attention SoftMax weights shape: torch.Size([BATCH_SIZE, HEAD_COUNT, SEQ_LEN, SEQ_LEN])
[MultiHeadAttention:6] Attention * V = Output shape after weighted sum: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])
[FeedForward] Input shape: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM]), Performing feedforward transformations.
[FeedForward] Output shape: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])
[TransformerDecoderBlock] out.shape: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])

[TransformerDecoder] Processing layer EMBEDDING_PER_HEAD
[TransformerDecoderBlock] x.shape: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM]), encoder_out.shape: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])
[MultiHeadAttention:1] Input shapes (V, K, Q) - values: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM]), keys: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM]), queries: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])
[MultiHeadAttention:2] After linear projections V/K/Q multihead- values: torch.Size([BATCH_SIZE, SEQ_LEN, HEAD_COUNT, EMBEDDING_PER_HEAD]), keys: torch.Size([BATCH_SIZE, SEQ_LEN, HEAD_COUNT, EMBEDDING_PER_HEAD]), queries: torch.Size([BATCH_SIZE, SEQ_LEN, HEAD_COUNT, EMBEDDING_PER_HEAD])
[MultiHeadAttention:3] Energy = Q * K (raw attention scores) shape: torch.Size([BATCH_SIZE, HEAD_COUNT, SEQ_LEN, SEQ_LEN])
[MultiHeadAttention:4] Applying mask with shape: torch.Size([BATCH_SIZE, 1, SEQ_LEN, SEQ_LEN])
[MultiHeadAttention:5] Attention SoftMax weights shape: torch.Size([BATCH_SIZE, HEAD_COUNT, SEQ_LEN, SEQ_LEN])
[MultiHeadAttention:6] Attention * V = Output shape after weighted sum: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])
[MultiHeadAttention:1] Input shapes (V, K, Q) - values: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM]), keys: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM]), queries: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])
[MultiHeadAttention:2] After linear projections V/K/Q multihead- values: torch.Size([BATCH_SIZE, SEQ_LEN, HEAD_COUNT, EMBEDDING_PER_HEAD]), keys: torch.Size([BATCH_SIZE, SEQ_LEN, HEAD_COUNT, EMBEDDING_PER_HEAD]), queries: torch.Size([BATCH_SIZE, SEQ_LEN, HEAD_COUNT, EMBEDDING_PER_HEAD])
[MultiHeadAttention:3] Energy = Q * K (raw attention scores) shape: torch.Size([BATCH_SIZE, HEAD_COUNT, SEQ_LEN, SEQ_LEN])
[MultiHeadAttention:4] Applying mask with shape: torch.Size([BATCH_SIZE, 1, 1, SEQ_LEN])
[MultiHeadAttention:5] Attention SoftMax weights shape: torch.Size([BATCH_SIZE, HEAD_COUNT, SEQ_LEN, SEQ_LEN])
[MultiHeadAttention:6] Attention * V = Output shape after weighted sum: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])
[FeedForward] Input shape: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM]), Performing feedforward transformations.
[FeedForward] Output shape: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])
[TransformerDecoderBlock] out.shape: torch.Size([BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM])
[TransformerDecoder] Output x.shape: torch.Size([BATCH_SIZE, SEQ_LEN, VOCAB_SIZE])
[Main] Final output shape: torch.Size([BATCH_SIZE, SEQ_LEN, VOCAB_SIZE])
